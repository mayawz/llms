# toy tutorial on attention mechanism in transformers
- self attention
- causal attention
- multihead (causal) attention (parallelized compute)
