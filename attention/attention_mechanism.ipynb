{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention mechanism toy tutorial\n",
    "- set up\n",
    "- self attention\n",
    "- causal attention\n",
    "- multihead attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'simple', 'toy', 'tutorial', 'on', 'how', 'attention', 'mechanism', 'works']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(0),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(7),\n",
       " tensor(8),\n",
       " tensor(9)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = \"This is simple toy tutorial on how attention mechanism works\"\n",
    "\n",
    "# import tiktoken\n",
    "# tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# tokens = tokenizer.encode(seq, allowed_special={'|eos|'})\n",
    "# print(tokens)\n",
    "\n",
    "seq_list = seq.split(\" \")\n",
    "print(seq_list)\n",
    "token_tensors = [torch.tensor(x) for x in range(len(seq_list))]\n",
    "token_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9182,  0.0033,  0.9627],\n",
       "        [-1.0063, -0.3444, -1.4247],\n",
       "        [ 1.1410,  0.3782, -0.5953],\n",
       "        [ 0.4299, -0.0343, -0.3688],\n",
       "        [-1.2274, -0.6004, -0.1838],\n",
       "        [ 0.4596, -0.0693, -1.5469],\n",
       "        [ 2.7773,  0.3163, -0.4481],\n",
       "        [-1.2793,  0.0345, -0.4968],\n",
       "        [-1.6882, -0.5491, -0.9991],\n",
       "        [-1.8605, -0.1361, -0.8169]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "# convert tokens into 3 dimensional embeddings\n",
    "embed = nn.Embedding(len(token_tensors), 3)\n",
    "input = torch.LongTensor(token_tensors)\n",
    "embeddings = embed(input)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### self attention\n",
    "- also called scaled dot-product attention\n",
    "- 3 steps: \n",
    "    1. attention score\n",
    "    2. normalization\n",
    "    3. context vector \n",
    "- make it with trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'simple', 'toy', 'tutorial', 'on', 'how', 'attention', 'mechanism', 'works']\n",
      "toy\n"
     ]
    }
   ],
   "source": [
    "print(seq_list)\n",
    "print(seq_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7499,  0.1047,  0.6971,  0.3220, -0.4393,  0.7705,  1.3484, -0.3680,\n",
      "        -0.3385, -0.4939], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "# calculate attention score\n",
    "# the dot product between each input token's embedding and each of the rest of the tokens' embeddings in the same sequence\n",
    "# we refer to the token of focus as query - here take \"toy\" as example, which is the indexed by 3 in the seq_list \n",
    "query = embeddings[3]\n",
    "attention_score_for_toy = torch.empty(embeddings.shape[0])\n",
    "for x in range(len(embeddings)):\n",
    "    attention_score_for_toy[x] = torch.dot(embeddings[x], query)\n",
    "print(attention_score_for_toy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7698, -0.4488, -1.6195, -0.7499,  0.9481, -1.9114, -2.9804,  0.6965,\n",
       "          0.5864,  0.9214],\n",
       "        [-0.4488,  3.1609, -0.4302,  0.1047,  1.7037,  1.7652, -2.2653,  1.9832,\n",
       "          3.3113,  3.0828],\n",
       "        [-1.6195, -0.4302,  1.7993,  0.6971, -1.5181,  1.4192,  3.5553, -1.1509,\n",
       "         -1.5391, -1.6880],\n",
       "        [-0.7499,  0.1047,  0.6971,  0.3220, -0.4393,  0.7705,  1.3484, -0.3680,\n",
       "         -0.3385, -0.4939],\n",
       "        [ 0.9481,  1.7037, -1.5181, -0.4393,  1.9008, -0.2383, -3.5165,  1.6408,\n",
       "          2.5854,  2.5154],\n",
       "        [-1.9114,  1.7652,  1.4192,  0.7705, -0.2383,  2.6089,  1.9478,  0.1780,\n",
       "          0.8076,  0.4179],\n",
       "        [-2.9804, -2.2653,  3.5553,  1.3484, -3.5165,  1.9478,  8.0142, -3.3196,\n",
       "         -4.4146, -4.8441],\n",
       "        [ 0.6965,  1.9832, -1.1509, -0.3680,  1.6408,  0.1780, -3.3196,  1.8847,\n",
       "          2.6371,  2.7813],\n",
       "        [ 0.5864,  3.3113, -1.5391, -0.3385,  2.5854,  0.8076, -4.4146,  2.6371,\n",
       "          4.1497,  4.0317],\n",
       "        [ 0.9214,  3.0828, -1.6880, -0.4939,  2.5154,  0.4179, -4.8441,  2.7813,\n",
       "          4.0317,  4.1471]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention scores' meaning:\n",
    "# dot product mathematically combines two vectors to yield a scalar value\n",
    "# dot product also measures similarity between the two vectors: the higher the more similar\n",
    "\n",
    "# in practice, rather than loop through each token as the query, we do matmul\n",
    "attention_scores = torch.matmul(embeddings, embeddings.T)\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.6066e-01, 3.9226e-02, 1.2165e-02, 2.9027e-02, 1.5857e-01, 9.0853e-03,\n",
       "         3.1196e-03, 1.2330e-01, 1.1045e-01, 1.5440e-01],\n",
       "        [6.7964e-03, 2.5115e-01, 6.9234e-03, 1.1820e-02, 5.8488e-02, 6.2197e-02,\n",
       "         1.1050e-03, 7.7349e-02, 2.9190e-01, 2.3227e-01],\n",
       "        [4.0431e-03, 1.3281e-02, 1.2346e-01, 4.1004e-02, 4.4746e-03, 8.4413e-02,\n",
       "         7.1471e-01, 6.4598e-03, 4.3818e-03, 3.7755e-03],\n",
       "        [3.4629e-02, 8.1389e-02, 1.4719e-01, 1.0115e-01, 4.7241e-02, 1.5839e-01,\n",
       "         2.8229e-01, 5.0735e-02, 5.2255e-02, 4.4732e-02],\n",
       "        [5.4625e-02, 1.1629e-01, 4.6378e-03, 1.3641e-02, 1.4162e-01, 1.6677e-02,\n",
       "         6.2869e-04, 1.0920e-01, 2.8083e-01, 2.6185e-01],\n",
       "        [3.8281e-03, 1.5126e-01, 1.0702e-01, 5.5941e-02, 2.0399e-02, 3.5169e-01,\n",
       "         1.8156e-01, 3.0933e-02, 5.8055e-02, 3.9317e-02],\n",
       "        [1.6540e-05, 3.3814e-05, 1.1401e-02, 1.2545e-03, 9.6760e-06, 2.2846e-03,\n",
       "         9.8498e-01, 1.1782e-05, 3.9412e-06, 2.5651e-06],\n",
       "        [3.7602e-02, 1.3614e-01, 5.9274e-03, 1.2969e-02, 9.6677e-02, 2.2388e-02,\n",
       "         6.7770e-04, 1.2337e-01, 2.6183e-01, 3.0241e-01],\n",
       "        [1.0019e-02, 1.5283e-01, 1.1960e-03, 3.9733e-03, 7.3953e-02, 1.2499e-02,\n",
       "         6.7435e-05, 7.7882e-02, 3.5346e-01, 3.1412e-01],\n",
       "        [1.4378e-02, 1.2484e-01, 1.0579e-03, 3.4917e-03, 7.0786e-02, 8.6895e-03,\n",
       "         4.5055e-05, 9.2344e-02, 3.2246e-01, 3.6191e-01]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize attention scores into attention weights\n",
    "# use torch.softmax() to avoid overflow/underflow and optimize compute\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to make sure attention scores sum up to 1 for each row\n",
    "attention_weights[3].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1575, -0.1810, -0.0689],\n",
       "        [-1.3100, -0.3126, -0.9870],\n",
       "        [ 2.1370,  0.2556, -0.5660],\n",
       "        [ 0.6602,  0.0412, -0.7017],\n",
       "        [-1.4214, -0.3106, -0.7217],\n",
       "        [ 0.4206, -0.0289, -1.0308],\n",
       "        [ 2.7501,  0.3157, -0.4522],\n",
       "        [-1.4282, -0.2850, -0.7887],\n",
       "        [-1.5255, -0.3317, -0.8916],\n",
       "        [-1.5547, -0.3089, -0.8561]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate context vector bease on attention weights and input embedding \n",
    "all_context_vecs = torch.matmul(attention_weights, embeddings)\n",
    "# all_context_vecs = attention_weights @ embeddings\n",
    "all_context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make it with trainable weights\n",
    "- instead of interacting directly with embeddings, now interact with transformed version\n",
    "- transformed by weight matrices: W_query, W_key, W_value\n",
    "    - query = search query in db; represent what the nn is now focused on understanding \n",
    "    - key = what is being indexed and searched; key to index each token in the context window \n",
    "    - value = value in key-value pair; \"attention\" finds the most relevant key to the query and retrieves the key's value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make two batches of training data (by stacking the embeddings for simplicity)\n",
    "from torch import nn\n",
    "class self_attention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        # inherit from the nn.Module parent \n",
    "        super().__init__() \n",
    "\n",
    "        # technically, we initialize the weight\n",
    "        # self.w_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        # in practice, we use the Linear layer with bias turned off to do so \n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # initialize the w_query, w_key, w_value \n",
    "        # AND matmul with input embeddings x\n",
    "        queries = self.w_query(x)\n",
    "        keys = self.w_key(x)\n",
    "        values = self.w_value(x)\n",
    "        # attention score query @ key.T\n",
    "        attention_scores = torch.matmul(queries, keys.T)\n",
    "        # attention weights = normalized attention scores\n",
    "        # scale the attention scores by the sqrt(embedding dimentsion) first \n",
    "        # to improve the training performance by avoiding small gradients.\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / (keys.shape[-1]**0.5),\n",
    "            dim=-1\n",
    "        )\n",
    "        # calculate context vector attention weights @ values\n",
    "        context_vectors = torch.matmul(attention_weights, values)\n",
    "        return context_vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2513, -0.1568],\n",
       "        [ 0.6291, -0.0860],\n",
       "        [ 0.3139, -0.1451],\n",
       "        [ 0.3260, -0.1417],\n",
       "        [ 0.4510, -0.1157],\n",
       "        [ 0.5232, -0.1044],\n",
       "        [ 0.0267, -0.2098],\n",
       "        [ 0.5554, -0.0983],\n",
       "        [ 0.6226, -0.0863],\n",
       "        [ 0.6407, -0.0842]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "# the example embeddings have dim 3\n",
    "d_in = embeddings.shape[-1]\n",
    "# usually in models like GPTs, d_in = d_out\n",
    "# here just for easy visualization\n",
    "d_out = embeddings.shape[-1]-1\n",
    "# generate a self attention object from the class\n",
    "self_attn = self_attention(d_in, d_out)\n",
    "# apply self_attn to embeddings\n",
    "self_attn(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### causal attention\n",
    "- mask out the future tokens that the current token has not seen \n",
    "- only look at the previous and current token in a context window\n",
    "- further apply a small portion of dropout (usually 0.1, 0.2)\n",
    "- re-applying softmax normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1035, 0.0925, 0.1075, 0.1049, 0.0971, 0.0995, 0.1187, 0.0947, 0.0910,\n",
      "         0.0905],\n",
      "        [0.0709, 0.1422, 0.0542, 0.0666, 0.1128, 0.0905, 0.0316, 0.1162, 0.1606,\n",
      "         0.1543],\n",
      "        [0.1002, 0.0996, 0.1016, 0.0996, 0.0976, 0.1001, 0.0998, 0.1015, 0.0987,\n",
      "         0.1012],\n",
      "        [0.0994, 0.1012, 0.0989, 0.0991, 0.1003, 0.1000, 0.0972, 0.1009, 0.1014,\n",
      "         0.1016],\n",
      "        [0.0899, 0.1172, 0.0802, 0.0883, 0.1097, 0.0986, 0.0668, 0.1066, 0.1237,\n",
      "         0.1189],\n",
      "        [0.0831, 0.1268, 0.0711, 0.0797, 0.1086, 0.0964, 0.0505, 0.1134, 0.1358,\n",
      "         0.1346],\n",
      "        [0.1106, 0.0687, 0.1344, 0.1147, 0.0788, 0.0937, 0.1903, 0.0802, 0.0627,\n",
      "         0.0660],\n",
      "        [0.0796, 0.1314, 0.0658, 0.0760, 0.1108, 0.0949, 0.0444, 0.1140, 0.1432,\n",
      "         0.1398],\n",
      "        [0.0717, 0.1414, 0.0548, 0.0677, 0.1143, 0.0909, 0.0328, 0.1149, 0.1598,\n",
      "         0.1516],\n",
      "        [0.0694, 0.1440, 0.0524, 0.0650, 0.1129, 0.0896, 0.0298, 0.1165, 0.1635,\n",
      "         0.1568]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# reuse the weights from the self attention\n",
    "queries = self_attn.w_query(embeddings)\n",
    "keys = self_attn.w_key(embeddings)\n",
    "attention_scores = torch.matmul(queries, keys.T)\n",
    "attention_weights = torch.softmax(\n",
    "            attention_scores / (keys.shape[-1]**0.5),\n",
    "            dim=-1\n",
    "        )\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "technically the mask use if manually re-normalize\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "practically the mask we use to take advantage of softmax(-inf)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# take vantage of the fact that softmax(-inf) approaches 0\n",
    "context_length = embeddings.shape[0]\n",
    "mask_original = torch.tril(torch.ones(context_length, context_length))\n",
    "print(\"technically the mask use if manually re-normalize\")\n",
    "print(mask_original)\n",
    "\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "print(\"practically the mask we use to take advantage of softmax(-inf)\")\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causally masked attention scores\n",
      "tensor([[-0.0087,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [ 0.0422,  1.0274,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [ 0.0033, -0.0055,  0.0226,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [ 0.0016,  0.0270, -0.0058, -0.0019,    -inf,    -inf,    -inf,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [ 0.0124,  0.3880, -0.1481, -0.0119,  0.2947,    -inf,    -inf,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [ 0.0280,  0.6264, -0.1917, -0.0309,  0.4076,  0.2384,    -inf,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [-0.0254, -0.7004,  0.2491,  0.0260, -0.5058, -0.2611,  0.7416,    -inf,\n",
      "            -inf,    -inf],\n",
      "        [ 0.0311,  0.7404, -0.2384, -0.0336,  0.4987,  0.2799, -0.7939,  0.5392,\n",
      "            -inf,    -inf],\n",
      "        [ 0.0390,  0.9997, -0.3394, -0.0411,  0.6986,  0.3752, -1.0649,  0.7066,\n",
      "          1.1733,    -inf],\n",
      "        [ 0.0442,  1.0769, -0.3523, -0.0475,  0.7334,  0.4063, -1.1525,  0.7774,\n",
      "          1.2566,  1.1977]], grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# then apply the mask where the mask is used as boolean values \n",
    "# when 1 fill with -inf\n",
    "masked = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(\"causally masked attention scores\")\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.3326, 0.6674, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.3325, 0.3304, 0.3371, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.2494, 0.2539, 0.2481, 0.2487, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1852, 0.2415, 0.1653, 0.1820, 0.2261, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1468, 0.2242, 0.1257, 0.1408, 0.1920, 0.1704, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1399, 0.0868, 0.1698, 0.1450, 0.0996, 0.1184, 0.2406, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1110, 0.1833, 0.0918, 0.1061, 0.1545, 0.1324, 0.0619, 0.1590, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0845, 0.1666, 0.0646, 0.0798, 0.1347, 0.1072, 0.0387, 0.1355, 0.1884,\n",
      "         0.0000],\n",
      "        [0.0694, 0.1440, 0.0524, 0.0650, 0.1129, 0.0896, 0.0298, 0.1165, 0.1635,\n",
      "         0.1568]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# causal attention weights\n",
    "causal_attention_weights = torch.softmax(\n",
    "    masked / keys.shape[-1]**0.5, \n",
    "    dim=1\n",
    "    )\n",
    "print(causal_attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spot check that each row still adds up to 1\n",
    "causal_attention_weights[3].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make two batches of training data (by stacking the embeddings for simplicity)\n",
    "from torch import nn\n",
    "class Causal_Attention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout_rate, qkv_bias=False):\n",
    "        # inherit from the nn.Module parent \n",
    "        super().__init__() \n",
    "\n",
    "        # technically, we initialize the weight\n",
    "        # self.w_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        # in practice, we use the Linear layer with bias turned off to do so \n",
    "        self.d_out = d_out\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # add the buffer to create mask and send it to device with the model \n",
    "        # but not update it\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(\n",
    "                torch.ones(context_length,context_length),\n",
    "                diagonal=1)\n",
    "        )\n",
    "        # add the dropout - object from nn.Dropout with param dropout_rate\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # allowing batching: first is the batch dim of tensors\n",
    "        batch, n_tokens, d_in = x.shape\n",
    "\n",
    "        # initialize the w_query, w_key, w_value \n",
    "        # AND matmul with input embeddings x\n",
    "        queries = self.w_query(x)\n",
    "        keys = self.w_key(x)\n",
    "        values = self.w_value(x)\n",
    "        # attention score query @ key.T \n",
    "        # but remember the first dim is now batch so transpose the second and third\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(1, 2))\n",
    "\n",
    "        # ###### add causal attention masks ######\n",
    "        # computeation with trailing underscore are performed in-place\n",
    "        attention_scores.masked_fill_(\n",
    "            # change the mask to boolean\n",
    "            self.mask.bool()[:n_tokens, :n_tokens],\n",
    "            # fill value when 1 in mask\n",
    "            -torch.inf\n",
    "        )\n",
    "        # ###### add causal attention masks ######\n",
    "\n",
    "        # attention weights = normalized attention scores\n",
    "        # scale the attention scores by the sqrt(embedding dimentsion) first \n",
    "        # to improve the training performance by avoiding small gradients.\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / (keys.shape[-1]**0.5),\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "        # ###### apply dropout to attention weights ######\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        # ###### apply dropout to attention weights ######\n",
    "\n",
    "        # calculate context vector attention weights @ values\n",
    "        context_vectors = torch.matmul(attention_weights, values)\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 3])\n"
     ]
    }
   ],
   "source": [
    "# to mimic actual training with batching \n",
    "# stack two embeddings as different batches\n",
    "batch = torch.stack((embeddings, embeddings), dim=0)\n",
    "# first dim shows the batch num\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 10, 2])\n"
     ]
    }
   ],
   "source": [
    "# try out Causal_Attention \n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "causal_attention = Causal_Attention(d_in, d_out, context_length, dropout_rate=0.1)\n",
    "context_vecs = causal_attention(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multihead attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write causal attention into a function \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
