{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention mechanism toy tutorial\n",
    "- set up\n",
    "- self attention\n",
    "- causal attention\n",
    "- multi-headed attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'simple', 'toy', 'tutorial', 'on', 'how', 'attention', 'mechanism', 'works']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(0),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(7),\n",
       " tensor(8),\n",
       " tensor(9)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = \"This is simple toy tutorial on how attention mechanism works\"\n",
    "\n",
    "# import tiktoken\n",
    "# tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# tokens = tokenizer.encode(seq, allowed_special={'|eos|'})\n",
    "# print(tokens)\n",
    "\n",
    "seq_list = seq.split(\" \")\n",
    "print(seq_list)\n",
    "token_tensors = [torch.tensor(x) for x in range(len(seq_list))]\n",
    "token_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9182,  0.0033,  0.9627],\n",
       "        [-1.0063, -0.3444, -1.4247],\n",
       "        [ 1.1410,  0.3782, -0.5953],\n",
       "        [ 0.4299, -0.0343, -0.3688],\n",
       "        [-1.2274, -0.6004, -0.1838],\n",
       "        [ 0.4596, -0.0693, -1.5469],\n",
       "        [ 2.7773,  0.3163, -0.4481],\n",
       "        [-1.2793,  0.0345, -0.4968],\n",
       "        [-1.6882, -0.5491, -0.9991],\n",
       "        [-1.8605, -0.1361, -0.8169]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "# convert tokens into 3 dimensional embeddings\n",
    "embed = nn.Embedding(len(tokens), 3)\n",
    "input = torch.LongTensor(token_tensors)\n",
    "embeddings = embed(input)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### self attention\n",
    "- also called scaled dot-product attention\n",
    "- 3 steps: \n",
    "    1. attention score\n",
    "    2. normalization\n",
    "    3. context vector \n",
    "- make it with trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'simple', 'toy', 'tutorial', 'on', 'how', 'attention', 'mechanism', 'works']\n",
      "toy\n"
     ]
    }
   ],
   "source": [
    "print(seq_list)\n",
    "print(seq_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7499,  0.1047,  0.6971,  0.3220, -0.4393,  0.7705,  1.3484, -0.3680,\n",
      "        -0.3385, -0.4939], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "# calculate attention score\n",
    "# the dot product between each input token's embedding and each of the rest of the tokens' embeddings in the same sequence\n",
    "# we refer to the token of focus as query - here take \"toy\" as example, which is the indexed by 3 in the seq_list \n",
    "query = embeddings[3]\n",
    "attention_score_for_toy = torch.empty(embeddings.shape[0])\n",
    "for x in range(len(embeddings)):\n",
    "    attention_score_for_toy[x] = torch.dot(embeddings[x], query)\n",
    "print(attention_score_for_toy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7698, -0.4488, -1.6195, -0.7499,  0.9481, -1.9114, -2.9804,  0.6965,\n",
       "          0.5864,  0.9214],\n",
       "        [-0.4488,  3.1609, -0.4302,  0.1047,  1.7037,  1.7652, -2.2653,  1.9832,\n",
       "          3.3113,  3.0828],\n",
       "        [-1.6195, -0.4302,  1.7993,  0.6971, -1.5181,  1.4192,  3.5553, -1.1509,\n",
       "         -1.5391, -1.6880],\n",
       "        [-0.7499,  0.1047,  0.6971,  0.3220, -0.4393,  0.7705,  1.3484, -0.3680,\n",
       "         -0.3385, -0.4939],\n",
       "        [ 0.9481,  1.7037, -1.5181, -0.4393,  1.9008, -0.2383, -3.5165,  1.6408,\n",
       "          2.5854,  2.5154],\n",
       "        [-1.9114,  1.7652,  1.4192,  0.7705, -0.2383,  2.6089,  1.9478,  0.1780,\n",
       "          0.8076,  0.4179],\n",
       "        [-2.9804, -2.2653,  3.5553,  1.3484, -3.5165,  1.9478,  8.0142, -3.3196,\n",
       "         -4.4146, -4.8441],\n",
       "        [ 0.6965,  1.9832, -1.1509, -0.3680,  1.6408,  0.1780, -3.3196,  1.8847,\n",
       "          2.6371,  2.7813],\n",
       "        [ 0.5864,  3.3113, -1.5391, -0.3385,  2.5854,  0.8076, -4.4146,  2.6371,\n",
       "          4.1497,  4.0317],\n",
       "        [ 0.9214,  3.0828, -1.6880, -0.4939,  2.5154,  0.4179, -4.8441,  2.7813,\n",
       "          4.0317,  4.1471]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention scores' meaning:\n",
    "# dot product mathematically combines two vectors to yield a scalar value\n",
    "# dot product also measures similarity between the two vectors: the higher the more similar\n",
    "\n",
    "# in practice, rather than loop through each token as the query, we do matmul\n",
    "attention_scores = torch.matmul(embeddings, embeddings.T)\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.6066e-01, 3.9226e-02, 1.2165e-02, 2.9027e-02, 1.5857e-01, 9.0853e-03,\n",
       "         3.1196e-03, 1.2330e-01, 1.1045e-01, 1.5440e-01],\n",
       "        [6.7964e-03, 2.5115e-01, 6.9234e-03, 1.1820e-02, 5.8488e-02, 6.2197e-02,\n",
       "         1.1050e-03, 7.7349e-02, 2.9190e-01, 2.3227e-01],\n",
       "        [4.0431e-03, 1.3281e-02, 1.2346e-01, 4.1004e-02, 4.4746e-03, 8.4413e-02,\n",
       "         7.1471e-01, 6.4598e-03, 4.3818e-03, 3.7755e-03],\n",
       "        [3.4629e-02, 8.1389e-02, 1.4719e-01, 1.0115e-01, 4.7241e-02, 1.5839e-01,\n",
       "         2.8229e-01, 5.0735e-02, 5.2255e-02, 4.4732e-02],\n",
       "        [5.4625e-02, 1.1629e-01, 4.6378e-03, 1.3641e-02, 1.4162e-01, 1.6677e-02,\n",
       "         6.2869e-04, 1.0920e-01, 2.8083e-01, 2.6185e-01],\n",
       "        [3.8281e-03, 1.5126e-01, 1.0702e-01, 5.5941e-02, 2.0399e-02, 3.5169e-01,\n",
       "         1.8156e-01, 3.0933e-02, 5.8055e-02, 3.9317e-02],\n",
       "        [1.6540e-05, 3.3814e-05, 1.1401e-02, 1.2545e-03, 9.6760e-06, 2.2846e-03,\n",
       "         9.8498e-01, 1.1782e-05, 3.9412e-06, 2.5651e-06],\n",
       "        [3.7602e-02, 1.3614e-01, 5.9274e-03, 1.2969e-02, 9.6677e-02, 2.2388e-02,\n",
       "         6.7770e-04, 1.2337e-01, 2.6183e-01, 3.0241e-01],\n",
       "        [1.0019e-02, 1.5283e-01, 1.1960e-03, 3.9733e-03, 7.3953e-02, 1.2499e-02,\n",
       "         6.7435e-05, 7.7882e-02, 3.5346e-01, 3.1412e-01],\n",
       "        [1.4378e-02, 1.2484e-01, 1.0579e-03, 3.4917e-03, 7.0786e-02, 8.6895e-03,\n",
       "         4.5055e-05, 9.2344e-02, 3.2246e-01, 3.6191e-01]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize attention scores into attention weights\n",
    "# use torch.softmax() to avoid overflow/underflow and optimize compute\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to make sure attention scores sum up to 1 for each row\n",
    "attention_weights[3].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1575, -0.1810, -0.0689],\n",
       "        [-1.3100, -0.3126, -0.9870],\n",
       "        [ 2.1370,  0.2556, -0.5660],\n",
       "        [ 0.6602,  0.0412, -0.7017],\n",
       "        [-1.4214, -0.3106, -0.7217],\n",
       "        [ 0.4206, -0.0289, -1.0308],\n",
       "        [ 2.7501,  0.3157, -0.4522],\n",
       "        [-1.4282, -0.2850, -0.7887],\n",
       "        [-1.5255, -0.3317, -0.8916],\n",
       "        [-1.5547, -0.3089, -0.8561]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate context vector bease on attention weights and input embedding \n",
    "all_context_vecs = torch.matmul(attention_weights, embeddings)\n",
    "# all_context_vecs = attention_weights @ embeddings\n",
    "all_context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make it with trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### causal attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi-headed attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
