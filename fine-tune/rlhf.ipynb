{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning From Human Feedback\n",
    "- 1. create a preference dataset\n",
    "    - give the LLM a piece of text to summarize, record the various  summarization as possible output set\n",
    "    - give the input, two of the summary from the set, and ask which of the two they prefer (more reliable measurement than give them all a scale 0-9 rating)\n",
    "    - then you get a preference dataset with a human labeler's preference between pairs of possible summaries\n",
    "    - Note - this is the preference of the human labelers not necessarily human preferences in general \n",
    "- 2. train a reward model with supervised learning\n",
    "    - usually the reward model itself is another LLM \n",
    "    - training:\n",
    "        - use the preference dataset, taking input of prompt, completion (summary) candidate 1, and candidate 2 to produce a score\n",
    "        - the loss function is trying to maximize the difference in score between the winning candidate and the losing candidate\n",
    "    - at inference time, we want this reward model to take any prompt and a completion and return a scalar indicating how good the completion is\n",
    "    - so essentially a regression models that output numbers (rather than categories indicators)\n",
    "- 3. use reward model in a RL loop to fine tune pre-trained LLM\n",
    "    - use another dataset - prompt dataset\n",
    "    - RL components:\n",
    "        - LLM = agent\n",
    "        - state: current LLM and context\n",
    "        - action: generate tokens\n",
    "    - each time a LLM generation a completion to a prompt, it receives a reward from the reward model indicating how aligned that generated completion to human preference\n",
    "    - learning the policy that maximize reward. the policy is learned via policy gradient method (proximal policy optimization, PPO, standard RL algorithm)\n",
    "    - base LLM model weights get updated via PPO\n",
    "    - also add penalty term to prevent the tuned model straying too far from the base LLM (KL-coefficient)\n",
    "- 4. evaluations\n",
    "    - rank-loss for the tuned llm\n",
    "    - kl-loss: increase and plateaus\n",
    "    - reward: increase and then plateau\n",
    "    - given a set of prompts and the final completions after tuning, have human qualitative feedbacks\n",
    "    - new research area: having other LLM generating preference dataset or having other LLM to do the final eval of the a RLHF tuned LLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install google-cloud-pipeline-components\n",
    "!pip3 install kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
