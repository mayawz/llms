{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.2 1B\n",
    "- all modules and utils are left in nb for educational purpose\n",
    "- model params (change from Llama 3.1 8B):\n",
    "    - vocab size 128256 -> 128256\n",
    "    - input embedding dim 4096 -> 2048\n",
    "    - context length 8192 -> 131_000\n",
    "    - masked grouped-query attention w/ 32 heads -> 32 heads\n",
    "    - n_layers of the the attention 32 -> 16\n",
    "    - RoPE -> RoPE scaling\n",
    "    - final feedfordward layer: Swish + SwiGLU+Linear as gate, hidden layer dim 14336 -> 8192\n",
    "    - add back weight tying:\n",
    "        reuses / shares weights between token -> input embedding layer and final feedfordward output layer\n",
    "- NOTE: \n",
    "    - GPT applies the positional embeddings to the inputs\n",
    "    - Llama applies rotations to the query and key vectors in the self-attention mechanism itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys \n",
    "import math\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "cwd = os.getcwd()\n",
    "base_path = cwd[:-13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA32_1B_CONFIG = {\n",
    "    \"vocab_size\": 128_256,      # Vocabulary size\n",
    "    \"context_length\": 131_072,  # Context length\n",
    "    \"emb_dim\": 2048,            # NEW: Half the embedding dimension\n",
    "    \"n_heads\": 32,              # Number of attention heads\n",
    "    \"n_layers\": 16,             # NEW: Half the number of layers\n",
    "    \"hidden_dim\": 8192,         # NEW: Almost half the size of the intermediate dimension in FeedForward\n",
    "    \"n_kv_groups\": 8,           # Key-Value groups for grouped-query attention\n",
    "    \"rope_base\": 50_000,        # The base in RoPE's \"theta\"\n",
    "    \"dtype\": torch.bfloat16,    # Lower-precision dtype to save memory\n",
    "    \"rope_freq\": {              # RoPE frequency scaling\n",
    "        \"factor\": 32.0,         # NEW: Adjustment of the rescaling factor\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"original_context_length\": 8192,\n",
    "    }\n",
    "}\n",
    "\n",
    "LLAMA32_3B_CONFIG = {\n",
    "    \"vocab_size\": 128_256,      # Vocabulary size\n",
    "    \"context_length\": 131_000,  # Context length\n",
    "    \"emb_dim\": 3072,            # Embedding dimension\n",
    "    \"n_heads\": 24,              # Number of attention heads\n",
    "    \"n_layers\": 28,             # Number of layers\n",
    "    \"hidden_dim\": 8192,         # Size of the intermediate dimension in FeedForward\n",
    "    \"n_kv_groups\": 8,           # Key-Value groups for grouped-query attention\n",
    "    \"rope_base\": 50_000,        # The base in RoPE's \"theta\"\n",
    "    \"dtype\": torch.bfloat16,    # Lower-precision dtype to save memory\n",
    "    \"rope_freq\": {              # RoPE frequency scaling\n",
    "        \"factor\": 32.0,\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"original_context_length\": 8192,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file name to import functions from \n",
    "from helper import import_defs_from_notebook\n",
    "fullname = \"converting-gpt-to-llama2\"\n",
    "names = [\"precompute_rope_params\", \"compute_rope\", \"SiLU\", \"FeedForward\", \"RMSNorm\", \"MultiHeadAttention\"]\n",
    "\n",
    "imported_module = import_defs_from_notebook(fullname, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to redefine precompute_rope_params\n",
    "# precompute_rope_params = getattr(imported_module, \"precompute_rope_params\", None)\n",
    "compute_rope = getattr(imported_module, \"compute_rope\", None)\n",
    "SiLU = getattr(imported_module, \"SiLU\", None)\n",
    "FeedForward = getattr(imported_module, \"FeedForward\", None)\n",
    "RMSNorm = getattr(imported_module, \"RMSNorm\", None)\n",
    "\n",
    "# MultiHeadAttention only for comparison purposes\n",
    "MultiHeadAttention = getattr(imported_module, \"MultiHeadAttention\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tie_weights(model):\n",
    "    \"\"\"\n",
    "    Ties the weights of the input embedding and output linear layers\n",
    "    ...\n",
    "        self.out_layer = nn.Linear(in_features=cfg[\"emb_dim\"],\n",
    "                                    out_features=cfg[\"vocab_size\"],\n",
    "                                    bias=False,\n",
    "                                    dtype=cfg[\"dtype\"])\n",
    "    ...\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_layer(x.to(torch.bfloat16))\n",
    "    \n",
    "    \"\"\"\n",
    "    model.lm_head.weight = model.embed_tokens.weight\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
