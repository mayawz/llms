{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify GPT2-124 to Llama 2-7B, 3-8B\n",
    "- all modules and utils are left in nb for educational purpose\n",
    "- Llama 2:\n",
    "    - vocab sizes: 50257 -> 32000\n",
    "    - input embedding dim 768 -> 4096\n",
    "    - positional encoding: absolute positional encoding -> RoPE\n",
    "    - context length 768 -> 4096\n",
    "    - remove dropout before & after multihead attention, and after final feedforward layer\n",
    "    - multihead causal attention w/ 12 atttention heads -> masked multihead attention w/ 32 heads\n",
    "    - layer norm -> RMSNorm (Root Mean Square Layer Normalization)\n",
    "    - final feedfordward layer: GELU -> Swish + SwiGLU+Linear as gate, hidden layer dim 11008\n",
    "- Llama 3:\n",
    "    - vocab size 32000 -> 128256\n",
    "    - input embedding dim 4096\n",
    "    - context length 4096 -> 8192\n",
    "    - masked multihead attention w/ 32 heads -> masked grouped-query attention w/ 32 heads\n",
    "    - final feedfordward layer: Swish + SwiGLU+Linear as gate, hidden layer dim 11008 -> 14336\n",
    "- NOTE: \n",
    "    - GPT applies the positional embeddings to the inputs\n",
    "    - Llama applies rotations to the query and key vectors in the self-attention mechanism itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys \n",
    "import math\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "cwd = os.getcwd()\n",
    "base_path = cwd[:-13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import load_api_key\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import hf_hub_download # get model weights\n",
    "import sentencepiece as spm # tokenizer \n",
    "\n",
    "HF_API_KEY = load_api_key(base_path, 'hf_llama2_at.txt')\n",
    "os.environ[\"HF_API_KEY\"] = HF_API_KEY\n",
    "login(token=HF_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer_file = hf_hub_download(\n",
    "    repo_id=\"meta-llama/Llama-2-7b\",\n",
    "    filename=\"tokenizer.model\",\n",
    "    local_dir=\"Llama-2-7B\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaTokenizer:\n",
    "    def __init__(self, filepath):\n",
    "        sp = spm.SentencePieceProcessor()\n",
    "        sp.load(tokenizer_file)\n",
    "        self.tokenizer = sp\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode_as_ids(text)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return self.tokenizer.decode_pieces(ids)\n",
    "\n",
    "\n",
    "tokenizer = LlamaTokenizer(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of the raw text: <class 'str'>\n",
      "The beginning of raw text: \n",
      " Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every un\n"
     ]
    }
   ],
   "source": [
    "# read in raw text\n",
    "pdata = f\"{base_path}traditional-NLP/data/\"\n",
    "sys.path.append(pdata)\n",
    "with open(f\"{pdata}anna.txt\" , 'r', encoding='utf-8') as f:\n",
    "    text_data = f.read()\n",
    "print(f\"The type of the raw text: {type(text_data)}\")\n",
    "print(f\"The beginning of raw text: \\n {text_data[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num of characters in Anna Karenina: 1985223\n",
      "total num of tokens in Anna Karenina with Sentencepiece tokenizer: 550383\n"
     ]
    }
   ],
   "source": [
    "# inspect raw text and tokens\n",
    "total_characters = len(text_data)\n",
    "print(f\"total num of characters in Anna Karenina: {total_characters}\")\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(f\"total num of tokens in Anna Karenina with Sentencepiece tokenizer: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9b7107ae30>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLAMA2_7B_CONFIG = {\n",
    "    \"vocab_size\": 32000,     # Vocabulary size\n",
    "    \"context_length\": 4096,  # Context length\n",
    "    \"emb_dim\": 4096,         # Embedding dimension\n",
    "    \"n_heads\": 32,           # Number of attention heads\n",
    "    \"n_layers\": 32,          # Number of layers\n",
    "    \"hidden_dim\": 11008,     # ADD: Size of the intermediate dimension in FeedForward\n",
    "    \"dtype\": torch.bfloat16  # ADD: Lower-precision dtype to save memory\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch dataset dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloader\n",
    "\n",
    "class my_text_dataset(Dataset):\n",
    "\n",
    "    # initialize with n varg in\n",
    "    def __init__(self, raw_text:str, tokenizer, max_length:int, stride:int=1):\n",
    "        # create class attributes\n",
    "        self.input_tokens_x = []\n",
    "        self.target_tokens_y = []\n",
    "\n",
    "        # tokenize the enitre text \n",
    "        tokens = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # set y as stride number of tokens trailing x \n",
    "        for i in range(0, (len(tokens)-max_length), stride):\n",
    "            x_tmp = tokens[i : (i+max_length)]\n",
    "            y_tmp = tokens[(i+1) : (i+max_length+1)]\n",
    "            self.input_tokens_x.append(torch.tensor(x_tmp))\n",
    "            self.target_tokens_y.append(torch.tensor(y_tmp))\n",
    "\n",
    "    # overwrite the __len__() method to return number of rows in the dataset\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the number of rows / pairs of x-y sequences in the dataset\"\n",
    "        return len(self.input_tokens_x)\n",
    "    \n",
    "    # overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "        return self.input_tokens_x[idx], self.target_tokens_y[idx]\n",
    "\n",
    "def my_text_dataloader(raw_text:str, batch_size:int=4, max_length:int=256, tokenizer=tokenizer,\n",
    "                       stride:int=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "\n",
    "    # create dataset\n",
    "    dataset = my_text_dataset(raw_text, tokenizer, max_length, stride)\n",
    "\n",
    "    # create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split into T, V, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_characters = len(text_data)\n",
    "print(f\"total num of characters in Anna Karenina: {total_characters}\")\n",
    "prop_t, prop_v, prop_h = (0.8,0.1,0.1)\n",
    "split_idx_t, split_idx_v = int(prop_t * total_characters), int((prop_t+prop_v) * total_characters)\n",
    "print(f\"Split at character index {split_idx_t} between train and valid sets, and at {split_idx_v} betwee valid and hold sets\")\n",
    "\n",
    "d_train = text_data[:split_idx_t]\n",
    "d_valid = text_data[split_idx_t:split_idx_v]\n",
    "d_hold  = text_data[split_idx_v:]\n",
    "\n",
    "assert (total_tokens * prop_t) > CONFIG_GPT2_124M[\"context_length\"], \"Not enough tokens for loader_t (training dataloader)\"\n",
    "assert (total_tokens * prop_v) > CONFIG_GPT2_124M[\"context_length\"], \"Not enough tokens for loader_v (validation dataloader)\"\n",
    "assert (total_tokens * prop_h) > CONFIG_GPT2_124M[\"context_length\"], \"Not enough tokens for loader_h (testing dataloader)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_t = my_text_dataloader(\n",
    "    raw_text=d_train,\n",
    "    batch_size=2, # this is only for learning purpose; in practice, batch_size >= 1024 is common\n",
    "    max_length=LLAMA2_7B_CONFIG[\"context_length\"],\n",
    "    stride=LLAMA2_7B_CONFIG[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "loader_v = my_text_dataloader(\n",
    "    raw_text=d_valid,\n",
    "    batch_size=2,\n",
    "    max_length=LLAMA2_7B_CONFIG[\"context_length\"],\n",
    "    stride=LLAMA2_7B_CONFIG[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# loader_h = my_text_dataloader(\n",
    "#     raw_text=d_hold,\n",
    "#     batch_size=2,\n",
    "#     max_length=LLAMA2_7B_CONFIG[\"context_length\"],\n",
    "#     stride=LLAMA2_7B_CONFIG[\"context_length\"],\n",
    "#     drop_last=False,\n",
    "#     shuffle=False,\n",
    "#     num_workers=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inspect loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in loader_t:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in loader_v:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modules and model\n",
    "- key components:\n",
    "    - tokenization - done in my_text_dataloader\n",
    "    - input embedding\n",
    "    - RoPE\n",
    "    - tansformer block\n",
    "        - layernorm\n",
    "        - multiheadattention CONFIG_GPT2_124M[\"n_heads\"] by CONFIG_GPT2_124M[\"n_layers\"]\n",
    "        - droppout+shortcut\n",
    "        - layernorm\n",
    "        - feedford\n",
    "        - dropout+shortcut\n",
    "    - layernorm\n",
    "    - output linear layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_rope_params(head_dim, theta_base=10_000, context_length=LLAMA2_7B_CONFIG['context_length']):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim // 2) / (head_dim // 2)))\n",
    "\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length)\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions[:, None] * inv_freq[None, :]  # Shape: (context_length, head_dim // 2)\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
    "\n",
    "    # Precompute sine and cosine\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "def compute_rope(x, cos, sin):\n",
    "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # Split x into first half and second half\n",
    "    x1 = x[..., : head_dim // 2]  # First half\n",
    "    x2 = x[..., head_dim // 2 :]  # Second half\n",
    "\n",
    "    # Adjust sin and cos shapes\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply the rotary transformation\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multihead Causal Attention with RoPE\n",
    "- hard code bias=False\n",
    "- set dtype for later reduced precision compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead_Causal_Attention_w_RoPE(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, n_heads, dtype=None): \n",
    "        # inherit from the nn.Module parent \n",
    "        super().__init__() \n",
    "\n",
    "        # make sure d_out is divisible by n_heads (modulous ope, remainder==0)\n",
    "        assert (d_out % n_heads == 0), \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_out // n_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        # add the buffer to create mask and send it to device with the model \n",
    "        # but not update it\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(\n",
    "                torch.ones(context_length,context_length),\n",
    "                diagonal=1)\n",
    "        )\n",
    "        # add linear layer to combine heads out\n",
    "        self.combine_heads = nn.Linear(d_out, d_out, bias=False, dtype=dtype)\n",
    "        \n",
    "        ################################### RoPE ###################################\n",
    "        cos, sin = precompute_rope_params(head_dim=self.head_dim, context_length=context_length)\n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin)\n",
    "        ###########################################################################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # allowing batching: first is the batch dim of tensors\n",
    "        batch, n_tokens, d_in = x.shape\n",
    "\n",
    "        # initialize the w_query, w_key, w_value \n",
    "        # AND matmul with input embeddings x\n",
    "        queries = self.w_query(x)\n",
    "        keys = self.w_key(x)\n",
    "        values = self.w_value(x)\n",
    "        # Shape: (b, num_tokens, d_out)\n",
    "\n",
    "        # ###### split weights for the heads ######\n",
    "        # dims from (batch, n_tokens, d_out) \n",
    "        # to (batch, n_tokens, n_heads, d_head)\n",
    "        queries = queries.view(batch, n_tokens, self.n_heads, self.d_head)\n",
    "        keys = keys.view(batch, n_tokens, self.n_heads, self.d_head)\n",
    "        values = values.view(batch, n_tokens, self.n_heads, self.d_head)\n",
    "\n",
    "        # then to (batch, n_heads, n_tokens, d_head)\n",
    "        queries = queries.transpose(1,2)\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        # ###### split weights for the heads ######\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        # apply RoPE for keys and queries\n",
    "        keys = compute_rope(keys, self.cos, self.sin)\n",
    "        queries = compute_rope(queries, self.cos, self.sin)\n",
    "        ###########################################################################\n",
    "\n",
    "        # attention score query @ key.T \n",
    "        # but remember the dims is (batch, n_tokens, n_heads, d_head) so transpose the last two\n",
    "        # !!! this computes dot product for each head !!!\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(2, 3))\n",
    "\n",
    "        # add causal attention masks \n",
    "        # computeation with trailing underscore are performed in-place\n",
    "        attention_scores.masked_fill_(\n",
    "            # change the mask to boolean (truncated to num of tokens)\n",
    "            self.mask.bool()[:n_tokens, :n_tokens],\n",
    "            # fill value when 1 in mask\n",
    "            -torch.inf\n",
    "        )\n",
    "\n",
    "        # attention weights = normalized attention scores\n",
    "        # scale the attention scores by the sqrt(embedding dimentsion) first \n",
    "        # to improve the training performance by avoiding small gradients.\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / (keys.shape[-1]**0.5),\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "        # dropout within attention is also removed\n",
    "\n",
    "        # calculate context vector attention weights @ values\n",
    "        # ###### combine across all heads  ######\n",
    "        # dims (batch, n_heads, n_tokens, d_head) to (batch, n_tokens, n_heads, d_head)\n",
    "        context_vectors = torch.matmul(attention_weights, values).transpose(1, 2)\n",
    "        context_vectors = context_vectors.contiguous().view(\n",
    "            batch, n_tokens, self.d_out\n",
    "        )\n",
    "        # Combines heads, where self.d_out= self.n_heads * self.d_head\n",
    "        context_vectors = self.combine_heads(context_vectors)\n",
    "        # ###### combine across all heads  ######\n",
    "\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing LayerNorm in GPT2\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.emb_dim = emb_dim\n",
    "        self.weight = nn.Parameter(torch.ones(emb_dim)).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        means = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        x_normed = x * torch.rsqrt(means + self.eps)\n",
    "        return (x_normed * self.weight).to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing GELU - use a Swish function\n",
    "# torch.nn.functional.silu works too\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeedForward with SwiGLU: Swish Gates Linear Unit\n",
    "class SwiGLU_FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        # self.silu = Swish()\n",
    "        self.silu = F.silu()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        # gated activation; element-wise multiplication\n",
    "        x = self.silu(x_fc1) * x_fc2\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Follows the architecture of llama2 \n",
    "\n",
    "    - tansformer block\n",
    "        - create residual_conn for attention\n",
    "        - rmsnorm\n",
    "        - multihead causal attention with rope\n",
    "        - shortcut for attention\n",
    "        - create residual_conn for swiglu-feedforward\n",
    "        - rmsnorm\n",
    "        - swiglu-feedforward\n",
    "        - shortcut for swiglu-feedforward\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = Multihead_Causal_Attention_w_RoPE(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            n_heads=cfg[\"n_heads\"],\n",
    "            dtype=cfg[\"dtype\"]  \n",
    "        )\n",
    "        self.ff = SwiGLU_FeedForward(cfg)\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        self.rmsnorm1 = RMSNorm(cfg[\"emb_dim\"])\n",
    "        self.rmsnorm2 = RMSNorm(cfg[\"emb_dim\"])\n",
    "        ###########################################################################\n",
    "\n",
    "        # self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define shortcut / residual connection for attenion block\n",
    "        residual_conn = x\n",
    "        # rms_norm before attention\n",
    "        x = self.rmsnorm1(x)\n",
    "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        # apply shortcut / residual connection\n",
    "        x = x + residual_conn  # Add the original input back\n",
    "\n",
    "        # define residual for FeedForward block\n",
    "        residual_conn = x\n",
    "        # rms_norm before swiglu-ff\n",
    "        x = self.rmsnorm2(x)\n",
    "        # swiglu-ff\n",
    "        x = self.ff(x)\n",
    "        # apply shortcut  \n",
    "        x = x + residual_conn  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it all together into a model \n",
    "class Llama2_model(nn.Module):\n",
    "    \"\"\"\n",
    "    - input embedding\n",
    "    - tansformer block\n",
    "    - rms_norm\n",
    "    - output linear layer\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.input_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "        # REMOVED absolute positional encoding\n",
    "\n",
    "        # transformer block x n_layers times\n",
    "        self.transformer_block = nn.Sequential(\n",
    "            # unpack list comprehension to repeat transformer-block n_layers times\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        # rsm_norm\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
    "        ###########################################################################\n",
    "        # final output layer\n",
    "        # expand tokens into logits in vocab_size dimensions\n",
    "        # do not add extra bias \n",
    "        self.out_layer = nn.Linear(in_features=cfg[\"emb_dim\"],\n",
    "                                   out_features=cfg[\"vocab_size\"],\n",
    "                                   bias=False,\n",
    "                                   dtype=cfg[\"dtype\"])\n",
    "        \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        input_embeddings = self.tok_emb(in_idx)\n",
    "        # REMOVE adding positional encoding\n",
    "        # transformer block repeated n_layers times\n",
    "        x = self.transformer_block(input_embeddings)\n",
    "        # rmsnorm\n",
    "        x = self.final_norm(x)\n",
    "        # final output layer -> logits\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
