{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify GPT2-124 to Llama 2-7B, 3-8B\n",
    "- Llama 2:\n",
    "    - vocab sizes: 50257 -> 32000\n",
    "    - input embedding dim 786 -> 4096\n",
    "    - positional encoding: absolute positional encoding -> RoPE\n",
    "    - context length 786 -> 4096\n",
    "    - remove dropout before & after multihead attention, and after final feedforward layer\n",
    "    - multihead causal attention w/ 12 atttention heads -> masked multihead attention w/ 32 heads\n",
    "    - layer norm -> RMS norm \n",
    "    - final feedfordward layer: GELU -> Swish + SwiGLU+Linear as gate, hidden layer dim 11008\n",
    "- Llama 3:\n",
    "    - vocab size 32000 -> 128256\n",
    "    - input embedding dim 4096\n",
    "    - context length 4096 -> 8192\n",
    "    - masked multihead attention w/ 32 heads -> masked grouped-query attention w/ 32 heads\n",
    "    - final feedfordward layer: Swish + SwiGLU+Linear as gate, hidden layer dim 11008 -> 14336\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub # get model weights\n",
    "import sentencepiece # tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys \n",
    "import math\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of the raw text: <class 'str'>\n",
      "The beginning of raw text: \n",
      " Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every un\n"
     ]
    }
   ],
   "source": [
    "# read in raw text\n",
    "pdata = f\"{cwd[:-18]}traditional-NLP/data/\"\n",
    "sys.path.append(pdata)\n",
    "with open(f\"{pdata}anna.txt\" , 'r', encoding='utf-8') as f:\n",
    "    text_data = f.read()\n",
    "print(f\"The type of the raw text: {type(text_data)}\")\n",
    "print(f\"The beginning of raw text: \\n {text_data[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num of characters in Anna Karenina: 1985223\n",
      "total num of tokens in Anna Karenina with BPE tokenizer: 508206\n"
     ]
    }
   ],
   "source": [
    "# inspect raw text and tokens\n",
    "total_characters = len(text_data)\n",
    "print(f\"total num of characters in Anna Karenina: {total_characters}\")\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(f\"total num of tokens in Anna Karenina with BPE tokenizer: {total_tokens}\")\n",
    "# total num of characters in Anna Karenina: 1985223\n",
    "# total num of tokens in Anna Karenina with BPE tokenizer: 508206"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch dataset dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloader\n",
    "\n",
    "class my_text_dataset(Dataset):\n",
    "\n",
    "    # initialize with n varg in\n",
    "    def __init__(self, raw_text:str, tokenizer, max_length:int, stride:int=1):\n",
    "        # create class attributes\n",
    "        self.input_tokens_x = []\n",
    "        self.target_tokens_y = []\n",
    "\n",
    "        # tokenize the enitre text \n",
    "        tokens = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # set y as stride number of tokens trailing x \n",
    "        for i in range(0, (len(tokens)-max_length), stride):\n",
    "            x_tmp = tokens[i : (i+max_length)]\n",
    "            y_tmp = tokens[(i+1) : (i+max_length+1)]\n",
    "            self.input_tokens_x.append(torch.tensor(x_tmp))\n",
    "            self.target_tokens_y.append(torch.tensor(y_tmp))\n",
    "\n",
    "    # overwrite the __len__() method to return number of rows in the dataset\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the number of rows / pairs of x-y sequences in the dataset\"\n",
    "        return len(self.input_tokens_x)\n",
    "    \n",
    "    # overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "        return self.input_tokens_x[idx], self.target_tokens_y[idx]\n",
    "\n",
    "def my_text_dataloader(raw_text:str, batch_size:int=4, max_length:int=256,\n",
    "                       stride:int=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # create dataset\n",
    "    dataset = my_text_dataset(raw_text, tokenizer, max_length, stride)\n",
    "\n",
    "    # create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split into T, V, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num of characters in Anna Karenina: 1985223\n",
      "Split at character index 1588178 between train and valid sets, and at 1786700 betwee valid and hold sets\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "print(f\"total num of characters in Anna Karenina: {total_characters}\")\n",
    "prop_t, prop_v, prop_h = (0.8,0.1,0.1)\n",
    "split_idx_t, split_idx_v = int(prop_t * total_characters), int((prop_t+prop_v) * total_characters)\n",
    "print(f\"Split at character index {split_idx_t} between train and valid sets, and at {split_idx_v} betwee valid and hold sets\")\n",
    "\n",
    "d_train = text_data[:split_idx_t]\n",
    "d_valid = text_data[split_idx_t:split_idx_v]\n",
    "d_hold  = text_data[split_idx_v:]\n",
    "\n",
    "assert (total_tokens * prop_t) > CONFIG_GPT2_124M[\"context_length\"], \"Not enough tokens for loader_t (training dataloader)\"\n",
    "assert (total_tokens * prop_v) > CONFIG_GPT2_124M[\"context_length\"], \"Not enough tokens for loader_v (validation dataloader)\"\n",
    "assert (total_tokens * prop_h) > CONFIG_GPT2_124M[\"context_length\"], \"Not enough tokens for loader_h (testing dataloader)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_t = my_text_dataloader(\n",
    "    raw_text=d_train,\n",
    "    batch_size=2, # this is only for learning purpose; in practice, batch_size >= 1024 is common\n",
    "    max_length=CONFIG_GPT2_124M[\"context_length\"],\n",
    "    stride=CONFIG_GPT2_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "loader_v = my_text_dataloader(\n",
    "    raw_text=d_valid,\n",
    "    batch_size=2,\n",
    "    max_length=CONFIG_GPT2_124M[\"context_length\"],\n",
    "    stride=CONFIG_GPT2_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# loader_h = my_text_dataloader(\n",
    "#     raw_text=d_hold,\n",
    "#     batch_size=2,\n",
    "#     max_length=CONFIG_GPT2_124M[\"context_length\"],\n",
    "#     stride=CONFIG_GPT2_124M[\"context_length\"],\n",
    "#     drop_last=False,\n",
    "#     shuffle=False,\n",
    "#     num_workers=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inspect loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 406528\n",
      "Validation tokens: 50944\n",
      "All tokens: 457472\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in loader_t:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in loader_v:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
