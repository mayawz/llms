{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify GPT2-124 to Llama 2-7B, 3-8B\n",
    "- all modules and utils are left in nb for educational purpose\n",
    "- Llama 2:\n",
    "    - vocab sizes: 50257 -> 32000\n",
    "    - input embedding dim 768 -> 4096\n",
    "    - positional encoding: absolute positional encoding -> RoPE\n",
    "    - context length 768 -> 4096\n",
    "    - remove dropout before & after multihead attention, and after final feedforward layer\n",
    "    - multihead causal attention w/ 12 atttention heads -> masked multihead attention w/ 32 heads\n",
    "    - layer norm -> RMSNorm (Root Mean Square Layer Normalization)\n",
    "    - final feedfordward layer: GELU -> Swish + SwiGLU+Linear as gate, hidden layer dim 11008\n",
    "- Llama 3 8B:\n",
    "    - vocab size 32000 -> 128256\n",
    "    - input embedding dim 4096\n",
    "    - context length 4096 -> 8192\n",
    "    - masked multihead attention w/ 32 heads -> masked grouped-query attention w/ 32 heads\n",
    "    - final feedfordward layer: Swish + SwiGLU+Linear as gate, hidden layer dim 11008 -> 14336\n",
    "- NOTE: \n",
    "    - GPT applies the positional embeddings to the inputs\n",
    "    - Llama applies rotations to the query and key vectors in the self-attention mechanism itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys \n",
    "import math\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "cwd = os.getcwd()\n",
    "base_path = cwd[:-13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import load_api_key\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import hf_hub_download # get model weights\n",
    "import sentencepiece as spm # tokenizer \n",
    "\n",
    "HF_API_KEY = load_api_key(base_path, 'hf_llama2_at.txt')\n",
    "os.environ[\"HF_API_KEY\"] = HF_API_KEY\n",
    "login(token=HF_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer_file = hf_hub_download(\n",
    "    repo_id=\"meta-llama/Llama-2-7b\",\n",
    "    filename=\"tokenizer.model\",\n",
    "    local_dir=\"Llama-2-7B\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaTokenizer:\n",
    "    def __init__(self, filepath):\n",
    "        sp = spm.SentencePieceProcessor()\n",
    "        sp.load(tokenizer_file)\n",
    "        self.tokenizer = sp\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode_as_ids(text)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return self.tokenizer.decode_pieces(ids)\n",
    "\n",
    "\n",
    "tokenizer = LlamaTokenizer(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of the raw text: <class 'str'>\n",
      "The beginning of raw text: \n",
      " Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every un\n"
     ]
    }
   ],
   "source": [
    "# read in raw text\n",
    "pdata = f\"{base_path}traditional-NLP/data/\"\n",
    "sys.path.append(pdata)\n",
    "with open(f\"{pdata}anna.txt\" , 'r', encoding='utf-8') as f:\n",
    "    text_data = f.read()\n",
    "print(f\"The type of the raw text: {type(text_data)}\")\n",
    "print(f\"The beginning of raw text: \\n {text_data[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num of characters in Anna Karenina: 1985223\n",
      "total num of tokens in Anna Karenina with Sentencepiece tokenizer: 550383\n"
     ]
    }
   ],
   "source": [
    "# inspect raw text and tokens\n",
    "total_characters = len(text_data)\n",
    "print(f\"total num of characters in Anna Karenina: {total_characters}\")\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(f\"total num of tokens in Anna Karenina with Sentencepiece tokenizer: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9b7107ae30>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLAMA2_7B_CONFIG = {\n",
    "    \"vocab_size\": 32000,     # Vocabulary size\n",
    "    \"context_length\": 4096,  # Context length\n",
    "    \"emb_dim\": 4096,         # Embedding dimension\n",
    "    \"n_heads\": 32,           # Number of attention heads\n",
    "    \"n_layers\": 32,          # Number of layers\n",
    "    \"hidden_dim\": 11008,     # ADD: Size of the intermediate dimension in FeedForward\n",
    "    \"dtype\": torch.bfloat16  # ADD: Lower-precision dtype to save memory\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch dataset dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloader\n",
    "\n",
    "class my_text_dataset(Dataset):\n",
    "\n",
    "    # initialize with n varg in\n",
    "    def __init__(self, raw_text:str, tokenizer, max_length:int, stride:int=1):\n",
    "        # create class attributes\n",
    "        self.input_tokens_x = []\n",
    "        self.target_tokens_y = []\n",
    "\n",
    "        # tokenize the enitre text \n",
    "        tokens = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # set y as stride number of tokens trailing x \n",
    "        for i in range(0, (len(tokens)-max_length), stride):\n",
    "            x_tmp = tokens[i : (i+max_length)]\n",
    "            y_tmp = tokens[(i+1) : (i+max_length+1)]\n",
    "            self.input_tokens_x.append(torch.tensor(x_tmp))\n",
    "            self.target_tokens_y.append(torch.tensor(y_tmp))\n",
    "\n",
    "    # overwrite the __len__() method to return number of rows in the dataset\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the number of rows / pairs of x-y sequences in the dataset\"\n",
    "        return len(self.input_tokens_x)\n",
    "    \n",
    "    # overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "        return self.input_tokens_x[idx], self.target_tokens_y[idx]\n",
    "\n",
    "def my_text_dataloader(raw_text:str, batch_size:int=4, max_length:int=256, tokenizer=tokenizer,\n",
    "                       stride:int=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "\n",
    "    # create dataset\n",
    "    dataset = my_text_dataset(raw_text, tokenizer, max_length, stride)\n",
    "\n",
    "    # create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split into T, V, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_characters = len(text_data)\n",
    "print(f\"total num of characters in Anna Karenina: {total_characters}\")\n",
    "prop_t, prop_v, prop_h = (0.8,0.1,0.1)\n",
    "split_idx_t, split_idx_v = int(prop_t * total_characters), int((prop_t+prop_v) * total_characters)\n",
    "print(f\"Split at character index {split_idx_t} between train and valid sets, and at {split_idx_v} betwee valid and hold sets\")\n",
    "\n",
    "d_train = text_data[:split_idx_t]\n",
    "d_valid = text_data[split_idx_t:split_idx_v]\n",
    "d_hold  = text_data[split_idx_v:]\n",
    "\n",
    "assert (total_tokens * prop_t) > CONFIG_GPT2_124M[\"context_length\"], \"Not enough tokens for loader_t (training dataloader)\"\n",
    "assert (total_tokens * prop_v) > CONFIG_GPT2_124M[\"context_length\"], \"Not enough tokens for loader_v (validation dataloader)\"\n",
    "assert (total_tokens * prop_h) > CONFIG_GPT2_124M[\"context_length\"], \"Not enough tokens for loader_h (testing dataloader)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_t = my_text_dataloader(\n",
    "    raw_text=d_train,\n",
    "    batch_size=2, # this is only for learning purpose; in practice, batch_size >= 1024 is common\n",
    "    max_length=LLAMA2_7B_CONFIG[\"context_length\"],\n",
    "    stride=LLAMA2_7B_CONFIG[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "loader_v = my_text_dataloader(\n",
    "    raw_text=d_valid,\n",
    "    batch_size=2,\n",
    "    max_length=LLAMA2_7B_CONFIG[\"context_length\"],\n",
    "    stride=LLAMA2_7B_CONFIG[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# loader_h = my_text_dataloader(\n",
    "#     raw_text=d_hold,\n",
    "#     batch_size=2,\n",
    "#     max_length=LLAMA2_7B_CONFIG[\"context_length\"],\n",
    "#     stride=LLAMA2_7B_CONFIG[\"context_length\"],\n",
    "#     drop_last=False,\n",
    "#     shuffle=False,\n",
    "#     num_workers=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inspect loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in loader_t:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in loader_v:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modules and model\n",
    "- key components:\n",
    "    - tokenization - done in my_text_dataloader\n",
    "    - input embedding\n",
    "    - RoPE\n",
    "    - tansformer block\n",
    "        - layernorm\n",
    "        - multiheadattention CONFIG_GPT2_124M[\"n_heads\"] by CONFIG_GPT2_124M[\"n_layers\"]\n",
    "        - droppout+shortcut\n",
    "        - layernorm\n",
    "        - feedford\n",
    "        - dropout+shortcut\n",
    "    - layernorm\n",
    "    - output linear layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_rope_params(head_dim, theta_base=10_000, context_length=LLAMA2_7B_CONFIG['context_length']):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim // 2) / (head_dim // 2)))\n",
    "\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length)\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions[:, None] * inv_freq[None, :]  # Shape: (context_length, head_dim // 2)\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
    "\n",
    "    # Precompute sine and cosine\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "def compute_rope(x, cos, sin):\n",
    "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # Split x into first half and second half\n",
    "    x1 = x[..., : head_dim // 2]  # First half\n",
    "    x2 = x[..., head_dim // 2 :]  # Second half\n",
    "\n",
    "    # Adjust sin and cos shapes\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply the rotary transformation\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multihead Causal Attention with RoPE\n",
    "- hard code bias=False\n",
    "- set dtype for later reduced precision compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead_Causal_Attention_w_RoPE(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, n_heads, dtype=None): \n",
    "        # inherit from the nn.Module parent \n",
    "        super().__init__() \n",
    "\n",
    "        # make sure d_out is divisible by n_heads (modulous ope, remainder==0)\n",
    "        assert (d_out % n_heads == 0), \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_out // n_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        # add the buffer to create mask and send it to device with the model \n",
    "        # but not update it\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(\n",
    "                torch.ones(context_length,context_length),\n",
    "                diagonal=1)\n",
    "        )\n",
    "        # add linear layer to combine heads out\n",
    "        self.combine_heads = nn.Linear(d_out, d_out, bias=False, dtype=dtype)\n",
    "        \n",
    "        ################################### RoPE ###################################\n",
    "        cos, sin = precompute_rope_params(head_dim=self.head_dim, context_length=context_length)\n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin)\n",
    "        ###########################################################################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # allowing batching: first is the batch dim of tensors\n",
    "        batch, n_tokens, d_in = x.shape\n",
    "\n",
    "        # initialize the w_query, w_key, w_value \n",
    "        # AND matmul with input embeddings x\n",
    "        queries = self.w_query(x)\n",
    "        keys = self.w_key(x)\n",
    "        values = self.w_value(x)\n",
    "        # Shape: (b, num_tokens, d_out)\n",
    "\n",
    "        # ###### split weights for the heads ######\n",
    "        # dims from (batch, n_tokens, d_out) \n",
    "        # to (batch, n_tokens, n_heads, d_head)\n",
    "        queries = queries.view(batch, n_tokens, self.n_heads, self.d_head)\n",
    "        keys = keys.view(batch, n_tokens, self.n_heads, self.d_head)\n",
    "        values = values.view(batch, n_tokens, self.n_heads, self.d_head)\n",
    "\n",
    "        # then to (batch, n_heads, n_tokens, d_head)\n",
    "        queries = queries.transpose(1,2)\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        # ###### split weights for the heads ######\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        # apply RoPE for keys and queries\n",
    "        keys = compute_rope(keys, self.cos, self.sin)\n",
    "        queries = compute_rope(queries, self.cos, self.sin)\n",
    "        ###########################################################################\n",
    "\n",
    "        # attention score query @ key.T \n",
    "        # but remember the dims is (batch, n_tokens, n_heads, d_head) so transpose the last two\n",
    "        # !!! this computes dot product for each head !!!\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(2, 3))\n",
    "\n",
    "        # add causal attention masks \n",
    "        # computeation with trailing underscore are performed in-place\n",
    "        attention_scores.masked_fill_(\n",
    "            # change the mask to boolean (truncated to num of tokens)\n",
    "            self.mask.bool()[:n_tokens, :n_tokens],\n",
    "            # fill value when 1 in mask\n",
    "            -torch.inf\n",
    "        )\n",
    "\n",
    "        # attention weights = normalized attention scores\n",
    "        # scale the attention scores by the sqrt(embedding dimentsion) first \n",
    "        # to improve the training performance by avoiding small gradients.\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / (keys.shape[-1]**0.5),\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "        # dropout within attention is also removed\n",
    "\n",
    "        # calculate context vector attention weights @ values\n",
    "        # ###### combine across all heads  ######\n",
    "        # dims (batch, n_heads, n_tokens, d_head) to (batch, n_tokens, n_heads, d_head)\n",
    "        context_vectors = torch.matmul(attention_weights, values).transpose(1, 2)\n",
    "        context_vectors = context_vectors.contiguous().view(\n",
    "            batch, n_tokens, self.d_out\n",
    "        )\n",
    "        # Combines heads, where self.d_out= self.n_heads * self.d_head\n",
    "        context_vectors = self.combine_heads(context_vectors)\n",
    "        # ###### combine across all heads  ######\n",
    "\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RMS_Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing LayerNorm in GPT2\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.emb_dim = emb_dim\n",
    "        self.weight = nn.Parameter(torch.ones(emb_dim)).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        means = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        x_normed = x * torch.rsqrt(means + self.eps)\n",
    "        return (x_normed * self.weight).to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Swish (SiLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing GELU - use a Swish function\n",
    "# torch.nn.functional.silu works too\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SwiGLU_FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeedForward with SwiGLU: Swish Gates Linear Unit\n",
    "class SwiGLU_FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        # self.silu = Swish()\n",
    "        # self.silu = F.silu()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        # gated activation; element-wise multiplication\n",
    "        x = F.silu(x_fc1) * x_fc2\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Follows the architecture of llama2 \n",
    "\n",
    "    - tansformer block\n",
    "        - create residual_conn for attention\n",
    "        - rmsnorm\n",
    "        - multihead causal attention with rope\n",
    "        - shortcut for attention\n",
    "        - create residual_conn for swiglu-feedforward\n",
    "        - rmsnorm\n",
    "        - swiglu-feedforward\n",
    "        - shortcut for swiglu-feedforward\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = Multihead_Causal_Attention_w_RoPE(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            n_heads=cfg[\"n_heads\"],\n",
    "            dtype=cfg[\"dtype\"]  \n",
    "        )\n",
    "        self.ff = SwiGLU_FeedForward(cfg)\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        self.rmsnorm1 = RMSNorm(cfg[\"emb_dim\"])\n",
    "        self.rmsnorm2 = RMSNorm(cfg[\"emb_dim\"])\n",
    "        ###########################################################################\n",
    "\n",
    "        # self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define shortcut / residual connection for attenion block\n",
    "        residual_conn = x\n",
    "        # rms_norm before attention\n",
    "        x = self.rmsnorm1(x)\n",
    "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        # apply shortcut / residual connection\n",
    "        x = x + residual_conn  # Add the original input back\n",
    "\n",
    "        # define residual for FeedForward block\n",
    "        residual_conn = x\n",
    "        # rms_norm before swiglu-ff\n",
    "        x = self.rmsnorm2(x)\n",
    "        # swiglu-ff\n",
    "        x = self.ff(x)\n",
    "        # apply shortcut  \n",
    "        x = x + residual_conn  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it all together into a model \n",
    "class Llama2_model(nn.Module):\n",
    "    \"\"\"\n",
    "    - input embedding\n",
    "    - tansformer block\n",
    "    - rms_norm\n",
    "    - output linear layer\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.input_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "        # REMOVED absolute positional encoding\n",
    "\n",
    "        # transformer block x n_layers times\n",
    "        self.transformer_block = nn.Sequential(\n",
    "            # unpack list comprehension to repeat transformer-block n_layers times\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        # rsm_norm\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
    "        ###########################################################################\n",
    "        # final output layer\n",
    "        # expand tokens into logits in vocab_size dimensions\n",
    "        # do not add extra bias \n",
    "        self.out_layer = nn.Linear(in_features=cfg[\"emb_dim\"],\n",
    "                                   out_features=cfg[\"vocab_size\"],\n",
    "                                   bias=False,\n",
    "                                   dtype=cfg[\"dtype\"])\n",
    "        \n",
    "    def forward(self, input_token):\n",
    "        batch_size, seq_len = input_token.shape\n",
    "        input_embeddings = self.input_emb(input_token)\n",
    "        # REMOVE adding positional encoding\n",
    "        # transformer block repeated n_layers times\n",
    "        x = self.transformer_block(input_embeddings)\n",
    "        # rmsnorm\n",
    "        x = self.final_norm(x)\n",
    "        # final output layer -> logits\n",
    "        logits = self.out_layer(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify to Llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA3_8B_CONFIG = {\n",
    "    \"vocab_size\": 128_256,   # NEW: Larger vocabulary size\n",
    "    \"context_length\": 8192,  # NEW: Larger context length\n",
    "    \"emb_dim\": 4096,         # Embedding dimension\n",
    "    \"n_heads\": 32,           # Number of attention heads\n",
    "    \"n_layers\": 32,          # Number of layers\n",
    "    \"hidden_dim\": 14_336,    # NEW: Larger size of the intermediate dimension in FeedForward\n",
    "    \"n_kv_groups\": 8,        # NEW: Key-Value groups for grouped-query attention\n",
    "    \"rope_base\": 50_000,     # NEW: The base in RoPE's \"theta\" was increased to 50_000\n",
    "    \"rope_freq\": None,       # NEW: Additional configuration for adjusting the RoPE frequencies\n",
    "    \"dtype\": torch.bfloat16  # Lower-precision dtype to save memory\n",
    "}\n",
    "\n",
    "LLAMA31_CONFIG_8B = {\n",
    "    \"vocab_size\": 128_256,      # Vocabulary size\n",
    "    \"context_length\": 131_072,  # NEW: Larger supported context length\n",
    "    \"emb_dim\": 4096,            # Embedding dimension\n",
    "    \"n_heads\": 32,              # Number of attention heads\n",
    "    \"n_layers\": 32,             # Number of layers\n",
    "    \"hidden_dim\": 14_336,       # Size of the intermediate dimension in FeedForward\n",
    "    \"n_kv_groups\": 8,           # Key-Value groups for grouped-query attention\n",
    "    \"rope_base\": 50_000,        # The base in RoPE's \"theta\"\n",
    "    \"dtype\": torch.bfloat16,    # Lower-precision dtype to save memory\n",
    "    \"rope_freq\": {              # NEW: RoPE frequency scaling\n",
    "        \"factor\": 8.0,\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"original_context_length\": 8192,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoPE scaling\n",
    "- Theta values are a set of predefined parameters that are used to determine the rotational angles in the rotary matrix, where d is the dimensionality of the embedding space\n",
    "- Increasing the base from 10,000 to 50,000 makes the frequencies (or rotation angles) decay more slowly across the dimensions, which means that higher dimensions will be associated with larger angles than before (essentially, it's a decompression of the frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_rope_params(head_dim, theta_base=10000, context_length=4096, freq_config=None):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim // 2) / (head_dim // 2)))\n",
    "\n",
    "    ################################ NEW ###############################################\n",
    "    # Frequency adjustments\n",
    "    if freq_config is not None:\n",
    "        low_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"low_freq_factor\"]\n",
    "        high_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"high_freq_factor\"]\n",
    "\n",
    "        wavelen = 2 * torch.pi / inv_freq\n",
    "\n",
    "        inv_freq_llama = torch.where(\n",
    "            wavelen > low_freq_wavelen, inv_freq / freq_config[\"factor\"], inv_freq\n",
    "        )\n",
    "\n",
    "        smooth_factor = (freq_config[\"original_context_length\"] / wavelen - freq_config[\"low_freq_factor\"]) / (\n",
    "            freq_config[\"high_freq_factor\"] - freq_config[\"low_freq_factor\"]\n",
    "        )\n",
    "\n",
    "        smoothed_inv_freq = (\n",
    "            (1 - smooth_factor) * (inv_freq / freq_config[\"factor\"]) + smooth_factor * inv_freq\n",
    "        )\n",
    "\n",
    "        is_medium_freq = (wavelen <= low_freq_wavelen) & (wavelen >= high_freq_wavelen)\n",
    "        inv_freq_llama = torch.where(is_medium_freq, smoothed_inv_freq, inv_freq_llama)\n",
    "        inv_freq = inv_freq_llama\n",
    "    ####################################################################################\n",
    "\n",
    "\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length)\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions[:, None] * inv_freq[None, :]  # Shape: (context_length, head_dim // 2)\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
    "\n",
    "    # Precompute sine and cosine\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouped-Query Attention\n",
    "- GQA can be thought of as a more compute- and parameter-efficient version of MHA\n",
    "- the main change in GQA is that each query group needs to be repeated to match the number of heads it is associated with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedBuffers:\n",
    "    \"\"\" \n",
    "    reuse the mask, cos, and sin tensors in the transformer blocks to improve efficiency (esp., when working with Llama 3.1 and 3.2 later with 131k context length)\n",
    "    \"\"\"\n",
    "    _buffers = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_buffers(context_length, head_dim, rope_base, freq_config, dtype=torch.float32):\n",
    "        key = (context_length, head_dim, rope_base, tuple(freq_config.values()) if freq_config else freq_config, dtype)\n",
    "\n",
    "        if key not in SharedBuffers._buffers:\n",
    "            # Create or fetch the buffers\n",
    "            mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "            cos, sin = precompute_rope_params(head_dim, rope_base, context_length, freq_config)\n",
    "            if dtype is not None:\n",
    "                cos = cos.to(dtype)\n",
    "                sin = sin.to(dtype)\n",
    "            SharedBuffers._buffers[key] = (mask, cos, sin)\n",
    "\n",
    "        return SharedBuffers._buffers[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self, d_in, d_out, context_length, num_heads,\n",
    "            num_kv_groups,       # NEW\n",
    "            rope_base=10_000,    # NEW\n",
    "            rope_config=None,    # NEW\n",
    "            dtype=None\n",
    "        ):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        ############################# NEW  #############################\n",
    "        # self.W_key = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        # self.W_value = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "        ################################################################\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)\n",
    "\n",
    "        ############################# NEW  #############################\n",
    "        # Fetch buffers using SharedBuffers\n",
    "        mask, cos, sin = SharedBuffers.get_buffers(context_length, self.head_dim, rope_base, rope_config, dtype)\n",
    "        ############################# NEW  #############################\n",
    "        \n",
    "        self.register_buffer(\"mask\", mask)\n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        queries = self.W_query(x)  # Shape: (b, num_tokens, d_out)\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, num_kv_groups * head_dim)\n",
    "        values = self.W_value(x)  # Shape: (b, num_tokens, num_kv_groups * head_dim)\n",
    "\n",
    "        # Reshape queries, keys, and values\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        ##################### NEW  #####################\n",
    "        # keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        # values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "        ################################################\n",
    "\n",
    "        # Transpose keys, values, and queries\n",
    "        keys = keys.transpose(1, 2)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        values = values.transpose(1, 2)  # Shape: (b, num_query_groups, num_tokens, head_dim)\n",
    "        queries = queries.transpose(1, 2)  # Shape: (b, num_query_groups, num_tokens, head_dim)\n",
    "\n",
    "        # Apply RoPE\n",
    "        keys = compute_rope(keys, self.cos, self.sin)\n",
    "        queries = compute_rope(queries, self.cos, self.sin)\n",
    "\n",
    "        ##################### NEW  #####################\n",
    "        # Expand keys and values to match the number of heads\n",
    "        # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        values = values.repeat_interleave(self.group_size, dim=1)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        # For example, before repeat_interleave along dim=1 (query groups):\n",
    "        #   [K1, K2]\n",
    "        # After repeat_interleave (each query group is repeated group_size times):\n",
    "        #   [K1, K1, K2, K2]\n",
    "        # If we used regular repeat instead of repeat_interleave, we'd get:\n",
    "        #   [K1, K2, K1, K2]\n",
    "        ################################################\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        # Shape: (b, num_heads, num_tokens, num_tokens)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        assert keys.shape[-1] == self.head_dim\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock_w_GQA(nn.Module):\n",
    "    \"\"\" \n",
    "    - create residual_conn for attention\n",
    "    - rmsnorm\n",
    "    - grouped query attention with rope\n",
    "    - shortcut for attention\n",
    "    - create residual_conn for swiglu-feedforward\n",
    "    - rmsnorm\n",
    "    - swiglu-feedforward\n",
    "    - shortcut for swiglu-feedforward\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att =  GroupedQueryAttention(  # MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            num_kv_groups=cfg[\"n_kv_groups\"],  # NEW\n",
    "            rope_base=cfg[\"rope_base\"],        # NEW\n",
    "            rope_config=cfg[\"rope_freq\"],      # NEW\n",
    "            dtype=cfg[\"dtype\"]\n",
    "        )\n",
    "        self.ff = SwiGLU_FeedForward(cfg)\n",
    "        self.rmsnorm1 = RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "        self.rmsnorm2 = RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define shortcut / residual connection for attenion block\n",
    "        residual_conn = x\n",
    "        x = self.rmsnorm1(x)\n",
    "        x = self.att(x.to(torch.bfloat16)) \n",
    "        x = x + residual_conn\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        residual_conn = x\n",
    "        x = self.rmsnorm2(x)\n",
    "        x = self.ff(x.to(torch.bfloat16))\n",
    "        x = x + residual_conn  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama3_model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.input_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock_w_GQA(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "        self.out_layer = nn.Linear(in_features=cfg[\"emb_dim\"],\n",
    "                                   out_features=cfg[\"vocab_size\"],\n",
    "                                   bias=False,\n",
    "                                   dtype=cfg[\"dtype\"])\n",
    "\n",
    "    def forward(self, input_token):\n",
    "        input_embeddings = self.input_emb(input_token)\n",
    "        x = self.trf_blocks(input_embeddings)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_layer(x.to(torch.bfloat16))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model size, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Llama3_model(LLAMA3_8B_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check if buffers are reused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(model.trf_blocks[0].att.mask is model.trf_blocks[-1].att.mask)\n",
    "print(model.trf_blocks[0].att.cos is model.trf_blocks[-1].att.cos)\n",
    "print(model.trf_blocks[0].att.sin is model.trf_blocks[-1].att.sin) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check total params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 8,030,261,248\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calc model memory size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (PyTorch default): 60.09 GB\n",
      "bfloat16: 30.04 GB\n"
     ]
    }
   ],
   "source": [
    "def model_memory_size(model, input_dtype=torch.float32):\n",
    "    total_params = 0\n",
    "    total_grads = 0\n",
    "    for param in model.parameters():\n",
    "        # Calculate total number of elements per parameter\n",
    "        param_size = param.numel()\n",
    "        total_params += param_size\n",
    "        # Check if gradients are stored for this parameter\n",
    "        if param.requires_grad:\n",
    "            total_grads += param_size\n",
    "\n",
    "    # Calculate buffer size (non-parameters that require memory)\n",
    "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
    "\n",
    "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
    "    # We assume parameters and gradients are stored in the same type as input dtype\n",
    "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
    "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
    "\n",
    "    # Convert bytes to gigabytes\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    return total_memory_gb\n",
    "\n",
    "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
    "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
