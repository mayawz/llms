{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training of GPT-2-124-million \n",
    "- pre-train on novel Anna Karenina\n",
    "- adaptations\n",
    "    - reduce the embedding size from 1024 to 256 to make sure the training loop will run locally\n",
    "    - change it back to 1024 when loading / using tre-trained weights\n",
    "- preprocess\n",
    "- modules and models\n",
    "    - define building-block modules\n",
    "    - define model\n",
    "- training loop & train-valid evaluation\n",
    "- save model params + optimizer, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys \n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.6.0\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer\n",
    "import importlib\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of the raw text: <class 'str'>\n",
      "The beginning of raw text: \n",
      " Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every un\n"
     ]
    }
   ],
   "source": [
    "# read in raw text\n",
    "pdata = f\"{cwd[:-18]}traditional-NLP/data/\"\n",
    "sys.path.append(pdata)\n",
    "with open(f\"{pdata}anna.txt\" , 'r', encoding='utf-8') as f:\n",
    "    text_data = f.read()\n",
    "print(f\"The type of the raw text: {type(text_data)}\")\n",
    "print(f\"The beginning of raw text: \\n {text_data[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num of characters in Anna Karenina: 1985223\n",
      "total num of tokens in Anna Karenina with BPE tokenizer: 508206\n"
     ]
    }
   ],
   "source": [
    "# inspect raw text and tokens\n",
    "total_characters = len(text_data)\n",
    "print(f\"total num of characters in Anna Karenina: {total_characters}\")\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(f\"total num of tokens in Anna Karenina with BPE tokenizer: {total_tokens}\")\n",
    "# total num of characters in Anna Karenina: 1985223\n",
    "# total num of tokens in Anna Karenina with BPE tokenizer: 508206"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f844e092e30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG_GPT2_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch dataset dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloader\n",
    "\n",
    "class my_text_dataset(Dataset):\n",
    "\n",
    "    # initialize with n varg in\n",
    "    def __init__(self, raw_text:str, tokenizer, max_length:int, stride:int=1):\n",
    "        # create class attributes\n",
    "        self.input_tokens_x = []\n",
    "        self.target_tokens_y = []\n",
    "\n",
    "        # tokenize the enitre text \n",
    "        tokens = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # set y as stride number of tokens trailing x \n",
    "        for i in range(0, (len(tokens)-max_length), stride):\n",
    "            x_tmp = tokens[i : (i+max_length)]\n",
    "            y_tmp = tokens[(i+1) : (i+max_length+1)]\n",
    "            self.input_tokens_x.append(torch.tensor(x_tmp))\n",
    "            self.target_tokens_y.append(torch.tensor(y_tmp))\n",
    "\n",
    "    # overwrite the __len__() method to return number of rows in the dataset\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the number of rows / pairs of x-y sequences in the dataset\"\n",
    "        return len(self.input_tokens_x)\n",
    "    \n",
    "    # overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "        return self.input_tokens_x[idx], self.target_tokens_y[idx]\n",
    "\n",
    "def my_text_dataloader(raw_text:str, batch_size:int=4, max_length:int=256,\n",
    "                       stride:int=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # create dataset\n",
    "    dataset = my_text_dataset(raw_text, tokenizer, max_length, stride)\n",
    "\n",
    "    # create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split into T, V, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num of characters in Anna Karenina: 1985223\n",
      "Split at character index 1588178 between train and valid sets, and at 1786700 betwee valid and hold sets\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "print(f\"total num of characters in Anna Karenina: {total_characters}\")\n",
    "prop_t, prop_v, prop_h = (0.8,0.1,0.1)\n",
    "split_idx_t, split_idx_v = int(prop_t * total_characters), int((prop_t+prop_v) * total_characters)\n",
    "print(f\"Split at character index {split_idx_t} between train and valid sets, and at {split_idx_v} betwee valid and hold sets\")\n",
    "\n",
    "d_train = text_data[:split_idx_t]\n",
    "d_valid = text_data[split_idx_t:split_idx_v]\n",
    "d_hold  = text_data[split_idx_v:]\n",
    "\n",
    "assert (total_tokens * prop_t) > CONFIG_GPT2_124M[\"context_length\"], \"Not enough tokens for loader_t (training dataloader)\"\n",
    "assert (total_tokens * prop_v) > CONFIG_GPT2_124M[\"context_length\"], \"Not enough tokens for loader_v (validation dataloader)\"\n",
    "assert (total_tokens * prop_h) > CONFIG_GPT2_124M[\"context_length\"], \"Not enough tokens for loader_h (testing dataloader)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_t = my_text_dataloader(\n",
    "    raw_text=d_train,\n",
    "    batch_size=2, # this is only for learning purpose; in practice, batch_size >= 1024 is common\n",
    "    max_length=CONFIG_GPT2_124M[\"context_length\"],\n",
    "    stride=CONFIG_GPT2_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "loader_v = my_text_dataloader(\n",
    "    raw_text=d_valid,\n",
    "    batch_size=2,\n",
    "    max_length=CONFIG_GPT2_124M[\"context_length\"],\n",
    "    stride=CONFIG_GPT2_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# loader_h = my_text_dataloader(\n",
    "#     raw_text=d_hold,\n",
    "#     batch_size=2,\n",
    "#     max_length=CONFIG_GPT2_124M[\"context_length\"],\n",
    "#     stride=CONFIG_GPT2_124M[\"context_length\"],\n",
    "#     drop_last=False,\n",
    "#     shuffle=False,\n",
    "#     num_workers=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inspect loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in loader_t:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in loader_v:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 406528\n",
      "Validation tokens: 50944\n",
      "All tokens: 457472\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in loader_t:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in loader_v:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modules and model\n",
    "- key components:\n",
    "    - tokenization - done in my_text_dataloader\n",
    "    - input embedding\n",
    "    - positional encoding\n",
    "    - dropout\n",
    "    - tansformer block\n",
    "        - layernorm\n",
    "        - multiheadattention CONFIG_GPT2_124M[\"n_heads\"] by CONFIG_GPT2_124M[\"n_layers\"]\n",
    "        - droppout+shortcut\n",
    "        - layernorm\n",
    "        - feedford\n",
    "        - dropout+shortcut\n",
    "    - layernorm\n",
    "    - output linear layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead_Causal_Attention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, n_heads, dropout_rate, qkv_bias=False):\n",
    "        # inherit from the nn.Module parent \n",
    "        super().__init__() \n",
    "\n",
    "        # make sure d_out is divisible by n_heads (modulous ope, remainder==0)\n",
    "        assert (d_out % n_heads == 0), \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.n_heads = n_heads\n",
    "        # floor division\n",
    "        self.d_head = d_out // n_heads\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # add the buffer to create mask and send it to device with the model \n",
    "        # but not update it\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(\n",
    "                torch.ones(context_length,context_length),\n",
    "                diagonal=1)\n",
    "        )\n",
    "        # add the dropout - object from nn.Dropout with param dropout_rate\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # add linear layer to combine heads out\n",
    "        self.combine_heads = nn.Linear(d_out, d_out)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # allowing batching: first is the batch dim of tensors\n",
    "        batch, n_tokens, d_in = x.shape\n",
    "\n",
    "        # initialize the w_query, w_key, w_value \n",
    "        # AND matmul with input embeddings x\n",
    "        queries = self.w_query(x)\n",
    "        keys = self.w_key(x)\n",
    "        values = self.w_value(x)\n",
    "\n",
    "        # ###### split weights for the heads ######\n",
    "        # dims from (batch, n_tokens, d_out) \n",
    "        # to (batch, n_tokens, n_heads, d_head)\n",
    "        queries = queries.view(batch, n_tokens, self.n_heads, self.d_head)\n",
    "        keys = keys.view(batch, n_tokens, self.n_heads, self.d_head)\n",
    "        values = values.view(batch, n_tokens, self.n_heads, self.d_head)\n",
    "        # then to (batch, n_heads, n_tokens, d_head)\n",
    "        queries = queries.transpose(1,2)\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        # ###### split weights for the heads ######\n",
    "\n",
    "        # attention score query @ key.T \n",
    "        # but remember the dims is (batch, n_tokens, n_heads, d_head) so transpose the last two\n",
    "        # !!! this computes dot product for each head !!!\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(2, 3))\n",
    "\n",
    "        # add causal attention masks \n",
    "        # computeation with trailing underscore are performed in-place\n",
    "        attention_scores.masked_fill_(\n",
    "            # change the mask to boolean (truncated to num of tokens)\n",
    "            self.mask.bool()[:n_tokens, :n_tokens],\n",
    "            # fill value when 1 in mask\n",
    "            -torch.inf\n",
    "        )\n",
    "\n",
    "        # attention weights = normalized attention scores\n",
    "        # scale the attention scores by the sqrt(embedding dimentsion) first \n",
    "        # to improve the training performance by avoiding small gradients.\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / (keys.shape[-1]**0.5),\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "        # apply dropout to attention weights \n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # calculate context vector attention weights @ values\n",
    "        # ###### combine across all heads  ######\n",
    "        # dims (batch, n_heads, n_tokens, d_head) to (batch, n_tokens, n_heads, d_head)\n",
    "        context_vectors = torch.matmul(attention_weights, values).transpose(1, 2)\n",
    "        context_vectors = context_vectors.contiguous().view(\n",
    "            batch, n_tokens, self.d_out\n",
    "        )\n",
    "        # Combines heads, where self.d_out= self.n_heads * self.d_head\n",
    "        context_vectors = self.combine_heads(context_vectors)\n",
    "        # ###### combine across all heads  ######\n",
    "\n",
    "\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.LayerNorm(emb_dim)\n",
    "# if we code it out it does the following\n",
    "# each mini-batch in the scenario is all the input embeddings of one context \n",
    "# mean and var came from calc across columns of the emsbeddings for each token\n",
    "# then scale and shif provides a linear transformation\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        # unlike buffers, Parameters will be updated\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.GELU()\n",
    "# when coding it out, it looks like the following\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed forward netword after multihead attention in each transformer block\n",
    "# why does this particular architecture have a 4 x expansion and shrinkage?\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(config[\"emb_dim\"], 4 * config[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * config[\"emb_dim\"], config[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Follows the architecture of GPT-2-124_million\n",
    "    Decoder only transformer\n",
    "\n",
    "    - tansformer block\n",
    "        - layernorm\n",
    "        - multiheadattention CONFIG_GPT2_124M[\"n_heads\"] by CONFIG_GPT2_124M[\"n_layers\"]\n",
    "        - droppout\n",
    "        - shortcut\n",
    "        - layernorm\n",
    "        - feedford\n",
    "        - dropout\n",
    "        - shortcut\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lnorm1 = LayerNorm(config[\"emb_dim\"])\n",
    "        self.mhca = Multihead_Causal_Attention(d_in=config[\"emb_dim\"], \n",
    "                                               d_out=config[\"emb_dim\"], \n",
    "                                               context_length=config[\"context_length\"], \n",
    "                                               n_heads=config[\"n_heads\"], \n",
    "                                               dropout_rate=config[\"drop_rate\"])\n",
    "        self.drop_out = nn.Dropout(p=config[\"drop_rate\"])\n",
    "        self.lnorm2 = LayerNorm(config[\"emb_dim\"])\n",
    "        self.ff = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define shortcut / residual connection for attenion block\n",
    "        residual_conn = x\n",
    "        # layer norm before attention\n",
    "        x = self.lnorm1(x)\n",
    "        # multihead causal attention\n",
    "        x = self.mhca(x)\n",
    "        # dropout \n",
    "        x = self.drop_out(x)\n",
    "        # shortcut / residual connection\n",
    "        x = x + residual_conn\n",
    "\n",
    "        # define residual for FeedForward block\n",
    "        residual_conn = x\n",
    "        # layer norm\n",
    "        x = self.lnorm2(x)\n",
    "        # feedforward\n",
    "        x = self.ff(x)\n",
    "        # drop_out\n",
    "        x = self.drop_out(x)\n",
    "        # residual connection\n",
    "        x = x + residual_conn\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it all together into a model \n",
    "class GPT2_124_model(nn.Module):\n",
    "    \"\"\"\n",
    "    - input embedding\n",
    "    - positional encoding\n",
    "    - dropout\n",
    "    - tansformer block\n",
    "    - layernorm\n",
    "    - output linear layer\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # input embedding\n",
    "        self.input_emb = nn.Embedding(num_embeddings=config[\"vocab_size\"],\n",
    "                                      embedding_dim=config[\"emb_dim\"])\n",
    "        # absolute positional encoding\n",
    "        self.pos_emb = nn.Embedding(num_embeddings=config[\"context_length\"], \n",
    "                                    embedding_dim=config[\"emb_dim\"])\n",
    "        # TO TRY RoPE\n",
    "        # from torchtune.modules import RotaryPositionalEmbeddings\n",
    "        # rope_dim = config[\"emb_dim\"]/config[\"n_head\"]\n",
    "        # self.pos_emb = RotaryPositionalEmbeddings(dim=rope_dim)\n",
    "\n",
    "        # dropout\n",
    "        self.drop_out = nn.Dropout(p=config[\"drop_rate\"])\n",
    "\n",
    "        # transformer block x n_layers times\n",
    "        self.transformer_block = nn.Sequential(\n",
    "            # unpack list comprehension to repeat transformer-block n_layers times\n",
    "            *[TransformerBlock(config) for _ in range(config[\"n_layers\"])]\n",
    "        )    \n",
    "\n",
    "        # layer norm\n",
    "        self.lnorm = LayerNorm(config['emb_dim'])\n",
    "\n",
    "        # final output layer\n",
    "        # expand tokens into logits in vocab_size dimensions\n",
    "        # do not add extra bias \n",
    "        self.out_layer = nn.Linear(in_features=config[\"emb_dim\"],\n",
    "                                   out_features=config[\"vocab_size\"],\n",
    "                                   bias=False)\n",
    "    \n",
    "    def forward(self, input_tokens):\n",
    "        batch_size, seq_len = input_tokens.shape\n",
    "        input_embeddings = self.input_emb(input_tokens)\n",
    "        posit_embeddings = self.pos_emb(torch.arange(seq_len, device=input_tokens.device))\n",
    "        # add positional encoding into input embedding \n",
    "        x = input_embeddings + posit_embeddings\n",
    "        # dropout\n",
    "        x = self.drop_out(x)\n",
    "        # transformer block\n",
    "        x = self.transformer_block(x)\n",
    "        # layer norm\n",
    "        x = self.lnorm(x)\n",
    "        # final output layer -> logits\n",
    "        logits = self.out_layer(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training loop\n",
    "- A typical training loop\n",
    "- loop through all training epochs\n",
    "    - within each epoch, loop through all batches (n_batches = train_size / batch_size)\n",
    "        - reset (from previous batch iter) the loss gradient to zero \n",
    "        - calculate loss on the current batch\n",
    "        - backpropagate loss gradient \n",
    "        - step to update weight and biases for next loop of training\n",
    "        - claculate training and validation losses\n",
    "        - visualize losses and sample texts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define training eval funcs\n",
    "- loss calculation under the hood:\n",
    "    - get the logits from the transformer output layer\n",
    "    - convert logits to probablities with softmax\n",
    "    - get target probabilities \n",
    "    - take (-1) * log (probabilties) [ log(prob)<0 so maximize to 0 or (-1)* to minimize to 0 ]\n",
    "    - take the cross entropy loss between predicted and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss per batch\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "# function to compute loss per loader of data during training and validation\n",
    "# as sum of loss across all batches in the dataloader / number of batches\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    # get the number of batches to compute the loss\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        # iterate over all batches in the dataloader unless num_batches is otherwise given\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    # compute loss across batches \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            # sum loss from each batch \n",
    "            total_loss += loss.item() \n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "# calculate avg loss across train set and valid set respecitively\n",
    "def evaluate_model(model, data_train, data_valid, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(data_train, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(data_valid, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "# encode example text to token ids\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "# decode example token ids to text \n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (Batch, Token) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        # until reacing max_new_tokens\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model,\n",
    "                  data_train,\n",
    "                  data_valid,\n",
    "                  optimizer,\n",
    "                  num_epochs,\n",
    "                  device,\n",
    "                  eval_frequency,\n",
    "                  eval_iter,\n",
    "                  start_context,\n",
    "                  tokenizer=tokenizer):\n",
    "    \"\"\"\n",
    "    - loop through all training epochs\n",
    "    - within each epoch, loop through all batches (n_batches = train_size / batch_size)\n",
    "        - reset (from previous batch iter) the loss gradient to zero \n",
    "        - calculate loss on the current batch\n",
    "        - backpropagate loss gradient \n",
    "        - step to update weight and biases for next loop of training\n",
    "        - claculate training and validation losses\n",
    "        - visualize losses and sample texts \n",
    "    \"\"\"\n",
    "    # declare lists to track losses\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # main training loop\n",
    "    # loop through all training epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # set model to training mode\n",
    "        model.train()\n",
    "        # loop through all batches \n",
    "        for input_batch, target_batch in data_train:\n",
    "            # reset (from previous batch iter) the loss gradient to zero \n",
    "            optimizer.zero_grad()\n",
    "            # calculate loss on the current batch\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            # backpropagate loss gradient \n",
    "            loss.backward()\n",
    "            # step to update weight and biases for next loop of training\n",
    "            optimizer.step()\n",
    "            # keep a count of the tokens seen \n",
    "            tokens_seen += input_batch.numel()\n",
    "            # keep a count of global iterations (across batches and epochs) taken\n",
    "            global_step += 1\n",
    "\n",
    "            # print out eval from train and valid data if global steps is divisible by eval_frequency\n",
    "            if global_step % eval_frequency == 0:\n",
    "                # get loss for eval_iter number of batches\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, data_train, data_valid, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Epoch {epoch+1} (Global Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        \n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Global Step 000000): Train loss 9.620, Val loss 9.692\n",
      "Epoch 1 (Global Step 000005): Train loss 8.346, Val loss 8.208\n",
      "Epoch 1 (Global Step 000010): Train loss 7.427, Val loss 7.329\n",
      "Epoch 1 (Global Step 000015): Train loss 7.128, Val loss 6.888\n",
      "Epoch 1 (Global Step 000020): Train loss 7.194, Val loss 6.750\n",
      "Epoch 1 (Global Step 000025): Train loss 6.770, Val loss 6.669\n",
      "Epoch 1 (Global Step 000030): Train loss 6.692, Val loss 6.553\n",
      "Epoch 1 (Global Step 000035): Train loss 6.676, Val loss 6.470\n",
      "Epoch 1 (Global Step 000040): Train loss 6.562, Val loss 6.437\n",
      "Epoch 1 (Global Step 000045): Train loss 6.539, Val loss 6.384\n",
      "Epoch 1 (Global Step 000050): Train loss 6.398, Val loss 6.339\n",
      "Epoch 1 (Global Step 000055): Train loss 6.604, Val loss 6.285\n",
      "Epoch 1 (Global Step 000060): Train loss 6.479, Val loss 6.194\n",
      "Epoch 1 (Global Step 000065): Train loss 6.333, Val loss 6.140\n",
      "Epoch 1 (Global Step 000070): Train loss 6.199, Val loss 6.132\n",
      "Epoch 1 (Global Step 000075): Train loss 6.310, Val loss 6.070\n",
      "Epoch 1 (Global Step 000080): Train loss 5.832, Val loss 6.029\n",
      "Epoch 1 (Global Step 000085): Train loss 6.190, Val loss 6.010\n",
      "Epoch 1 (Global Step 000090): Train loss 5.912, Val loss 6.017\n",
      "Epoch 1 (Global Step 000095): Train loss 6.274, Val loss 5.959\n",
      "Epoch 1 (Global Step 000100): Train loss 6.066, Val loss 5.840\n",
      "Epoch 1 (Global Step 000105): Train loss 5.955, Val loss 5.800\n",
      "Epoch 1 (Global Step 000110): Train loss 6.106, Val loss 5.817\n",
      "Epoch 1 (Global Step 000115): Train loss 5.904, Val loss 5.749\n",
      "Epoch 1 (Global Step 000120): Train loss 5.889, Val loss 5.726\n",
      "Epoch 1 (Global Step 000125): Train loss 5.907, Val loss 5.746\n",
      "Epoch 1 (Global Step 000130): Train loss 5.924, Val loss 5.709\n",
      "Epoch 1 (Global Step 000135): Train loss 5.829, Val loss 5.691\n",
      "Epoch 1 (Global Step 000140): Train loss 5.746, Val loss 5.677\n",
      "Epoch 1 (Global Step 000145): Train loss 5.668, Val loss 5.688\n",
      "Epoch 1 (Global Step 000150): Train loss 5.546, Val loss 5.634\n",
      "Epoch 1 (Global Step 000155): Train loss 5.653, Val loss 5.590\n",
      "Epoch 1 (Global Step 000160): Train loss 5.739, Val loss 5.527\n",
      "Epoch 1 (Global Step 000165): Train loss 5.784, Val loss 5.473\n",
      "Epoch 1 (Global Step 000170): Train loss 5.936, Val loss 5.441\n",
      "Epoch 1 (Global Step 000175): Train loss 5.958, Val loss 5.437\n",
      "Epoch 1 (Global Step 000180): Train loss 5.493, Val loss 5.452\n",
      "Epoch 1 (Global Step 000185): Train loss 5.480, Val loss 5.415\n",
      "Epoch 1 (Global Step 000190): Train loss 5.785, Val loss 5.394\n",
      "Epoch 1 (Global Step 000195): Train loss 5.839, Val loss 5.415\n",
      "Epoch 1 (Global Step 000200): Train loss 5.684, Val loss 5.413\n",
      "Epoch 1 (Global Step 000205): Train loss 5.768, Val loss 5.443\n",
      "Epoch 1 (Global Step 000210): Train loss 5.787, Val loss 5.424\n",
      "Epoch 1 (Global Step 000215): Train loss 5.714, Val loss 5.350\n",
      "Epoch 1 (Global Step 000220): Train loss 5.557, Val loss 5.347\n",
      "Epoch 1 (Global Step 000225): Train loss 5.518, Val loss 5.335\n",
      "Epoch 1 (Global Step 000230): Train loss 5.286, Val loss 5.284\n",
      "Epoch 1 (Global Step 000235): Train loss 5.511, Val loss 5.266\n",
      "Epoch 1 (Global Step 000240): Train loss 5.468, Val loss 5.245\n",
      "Epoch 1 (Global Step 000245): Train loss 5.656, Val loss 5.248\n",
      "Epoch 1 (Global Step 000250): Train loss 5.562, Val loss 5.269\n",
      "Epoch 1 (Global Step 000255): Train loss 5.710, Val loss 5.279\n",
      "Epoch 1 (Global Step 000260): Train loss 5.771, Val loss 5.242\n",
      "Epoch 1 (Global Step 000265): Train loss 5.575, Val loss 5.204\n",
      "Epoch 1 (Global Step 000270): Train loss 5.261, Val loss 5.232\n",
      "Epoch 1 (Global Step 000275): Train loss 5.488, Val loss 5.232\n",
      "Epoch 1 (Global Step 000280): Train loss 5.463, Val loss 5.241\n",
      "Epoch 1 (Global Step 000285): Train loss 5.382, Val loss 5.230\n",
      "Epoch 1 (Global Step 000290): Train loss 5.503, Val loss 5.221\n",
      "Epoch 1 (Global Step 000295): Train loss 5.614, Val loss 5.241\n",
      "Epoch 1 (Global Step 000300): Train loss 5.444, Val loss 5.238\n",
      "Epoch 1 (Global Step 000305): Train loss 5.307, Val loss 5.205\n",
      "Epoch 1 (Global Step 000310): Train loss 5.260, Val loss 5.225\n",
      "Epoch 1 (Global Step 000315): Train loss 5.216, Val loss 5.215\n",
      "Epoch 1 (Global Step 000320): Train loss 5.346, Val loss 5.183\n",
      "Epoch 1 (Global Step 000325): Train loss 5.543, Val loss 5.165\n",
      "Epoch 1 (Global Step 000330): Train loss 5.435, Val loss 5.134\n",
      "Epoch 1 (Global Step 000335): Train loss 5.281, Val loss 5.130\n",
      "Epoch 1 (Global Step 000340): Train loss 5.543, Val loss 5.140\n",
      "Epoch 1 (Global Step 000345): Train loss 5.130, Val loss 5.102\n",
      "Epoch 1 (Global Step 000350): Train loss 5.398, Val loss 5.113\n",
      "Epoch 1 (Global Step 000355): Train loss 5.539, Val loss 5.123\n",
      "Epoch 1 (Global Step 000360): Train loss 5.345, Val loss 5.113\n",
      "Epoch 1 (Global Step 000365): Train loss 5.391, Val loss 5.078\n",
      "Epoch 1 (Global Step 000370): Train loss 5.306, Val loss 5.081\n",
      "Epoch 1 (Global Step 000375): Train loss 5.108, Val loss 5.091\n",
      "Epoch 1 (Global Step 000380): Train loss 5.293, Val loss 5.077\n",
      "Epoch 1 (Global Step 000385): Train loss 5.229, Val loss 5.120\n",
      "Epoch 1 (Global Step 000390): Train loss 5.218, Val loss 5.099\n",
      "Epoch 1 (Global Step 000395): Train loss 5.417, Val loss 5.093\n",
      "Epoch 1 (Global Step 000400): Train loss 5.276, Val loss 5.081\n",
      "Epoch 1 (Global Step 000405): Train loss 5.151, Val loss 5.063\n",
      "Epoch 1 (Global Step 000410): Train loss 5.266, Val loss 5.070\n",
      "Epoch 1 (Global Step 000415): Train loss 5.279, Val loss 5.080\n",
      "Epoch 1 (Global Step 000420): Train loss 5.369, Val loss 5.056\n",
      "Epoch 1 (Global Step 000425): Train loss 5.379, Val loss 5.031\n",
      "Epoch 1 (Global Step 000430): Train loss 5.439, Val loss 5.014\n",
      "Epoch 1 (Global Step 000435): Train loss 5.207, Val loss 5.074\n",
      "Epoch 1 (Global Step 000440): Train loss 5.340, Val loss 5.072\n",
      "Epoch 1 (Global Step 000445): Train loss 5.194, Val loss 5.031\n",
      "Epoch 1 (Global Step 000450): Train loss 5.293, Val loss 5.005\n",
      "Epoch 1 (Global Step 000455): Train loss 5.011, Val loss 4.997\n",
      "Epoch 1 (Global Step 000460): Train loss 5.440, Val loss 5.024\n",
      "Epoch 1 (Global Step 000465): Train loss 5.072, Val loss 5.030\n",
      "Epoch 1 (Global Step 000470): Train loss 5.101, Val loss 5.006\n",
      "Epoch 1 (Global Step 000475): Train loss 5.216, Val loss 4.989\n",
      "Epoch 1 (Global Step 000480): Train loss 5.257, Val loss 4.997\n",
      "Epoch 1 (Global Step 000485): Train loss 5.183, Val loss 4.985\n",
      "Epoch 1 (Global Step 000490): Train loss 5.251, Val loss 4.947\n",
      "Epoch 1 (Global Step 000495): Train loss 5.100, Val loss 4.940\n",
      "Epoch 1 (Global Step 000500): Train loss 5.232, Val loss 4.929\n",
      "Epoch 1 (Global Step 000505): Train loss 5.266, Val loss 4.950\n",
      "Epoch 1 (Global Step 000510): Train loss 5.142, Val loss 4.948\n",
      "Epoch 1 (Global Step 000515): Train loss 4.961, Val loss 4.933\n",
      "Epoch 1 (Global Step 000520): Train loss 5.013, Val loss 4.941\n",
      "Epoch 1 (Global Step 000525): Train loss 5.008, Val loss 4.945\n",
      "Epoch 1 (Global Step 000530): Train loss 5.120, Val loss 4.963\n",
      "Epoch 1 (Global Step 000535): Train loss 5.211, Val loss 4.971\n",
      "Epoch 1 (Global Step 000540): Train loss 4.972, Val loss 4.955\n",
      "Epoch 1 (Global Step 000545): Train loss 4.870, Val loss 4.946\n",
      "Epoch 1 (Global Step 000550): Train loss 5.256, Val loss 4.902\n",
      "Epoch 1 (Global Step 000555): Train loss 5.444, Val loss 4.912\n",
      "Epoch 1 (Global Step 000560): Train loss 4.916, Val loss 4.892\n",
      "Epoch 1 (Global Step 000565): Train loss 5.007, Val loss 4.908\n",
      "Epoch 1 (Global Step 000570): Train loss 5.174, Val loss 4.960\n",
      "Epoch 1 (Global Step 000575): Train loss 4.997, Val loss 4.897\n",
      "Epoch 1 (Global Step 000580): Train loss 5.125, Val loss 4.855\n",
      "Epoch 1 (Global Step 000585): Train loss 5.171, Val loss 4.881\n",
      "Epoch 1 (Global Step 000590): Train loss 5.114, Val loss 4.867\n",
      "Epoch 1 (Global Step 000595): Train loss 5.391, Val loss 4.836\n",
      "Epoch 1 (Global Step 000600): Train loss 4.996, Val loss 4.861\n",
      "Epoch 1 (Global Step 000605): Train loss 5.195, Val loss 4.873\n",
      "Epoch 1 (Global Step 000610): Train loss 5.314, Val loss 4.850\n",
      "Epoch 1 (Global Step 000615): Train loss 5.304, Val loss 4.861\n",
      "Epoch 1 (Global Step 000620): Train loss 5.069, Val loss 4.872\n",
      "Epoch 1 (Global Step 000625): Train loss 5.158, Val loss 4.883\n",
      "Epoch 1 (Global Step 000630): Train loss 5.235, Val loss 4.862\n",
      "Epoch 1 (Global Step 000635): Train loss 4.986, Val loss 4.839\n",
      "Epoch 1 (Global Step 000640): Train loss 4.941, Val loss 4.839\n",
      "Epoch 1 (Global Step 000645): Train loss 4.948, Val loss 4.842\n",
      "Epoch 1 (Global Step 000650): Train loss 5.092, Val loss 4.853\n",
      "Epoch 1 (Global Step 000655): Train loss 4.936, Val loss 4.892\n",
      "Epoch 1 (Global Step 000660): Train loss 5.182, Val loss 4.885\n",
      "Epoch 1 (Global Step 000665): Train loss 5.120, Val loss 4.864\n",
      "Epoch 1 (Global Step 000670): Train loss 4.945, Val loss 4.858\n",
      "Epoch 1 (Global Step 000675): Train loss 5.238, Val loss 4.852\n",
      "Epoch 1 (Global Step 000680): Train loss 5.016, Val loss 4.845\n",
      "Epoch 1 (Global Step 000685): Train loss 4.868, Val loss 4.819\n",
      "Epoch 1 (Global Step 000690): Train loss 4.857, Val loss 4.804\n",
      "Epoch 1 (Global Step 000695): Train loss 4.953, Val loss 4.813\n",
      "Epoch 1 (Global Step 000700): Train loss 5.296, Val loss 4.836\n",
      "Epoch 1 (Global Step 000705): Train loss 5.082, Val loss 4.813\n",
      "Epoch 1 (Global Step 000710): Train loss 4.808, Val loss 4.791\n",
      "Epoch 1 (Global Step 000715): Train loss 4.961, Val loss 4.802\n",
      "Epoch 1 (Global Step 000720): Train loss 5.057, Val loss 4.796\n",
      "Epoch 1 (Global Step 000725): Train loss 5.007, Val loss 4.807\n",
      "Epoch 1 (Global Step 000730): Train loss 5.008, Val loss 4.784\n",
      "Epoch 1 (Global Step 000735): Train loss 5.039, Val loss 4.784\n",
      "Epoch 1 (Global Step 000740): Train loss 4.928, Val loss 4.778\n",
      "Epoch 1 (Global Step 000745): Train loss 4.849, Val loss 4.808\n",
      "Epoch 1 (Global Step 000750): Train loss 4.837, Val loss 4.785\n",
      "Epoch 1 (Global Step 000755): Train loss 4.963, Val loss 4.783\n",
      "Epoch 1 (Global Step 000760): Train loss 4.982, Val loss 4.769\n",
      "Epoch 1 (Global Step 000765): Train loss 4.895, Val loss 4.798\n",
      "Epoch 1 (Global Step 000770): Train loss 4.934, Val loss 4.820\n",
      "Epoch 1 (Global Step 000775): Train loss 5.082, Val loss 4.803\n",
      "Epoch 1 (Global Step 000780): Train loss 4.774, Val loss 4.783\n",
      "Epoch 1 (Global Step 000785): Train loss 4.757, Val loss 4.787\n",
      "Epoch 1 (Global Step 000790): Train loss 5.036, Val loss 4.780\n",
      "This is a test of the modified                                                   \n",
      "Epoch 2 (Global Step 000795): Train loss 4.921, Val loss 4.819\n",
      "Epoch 2 (Global Step 000800): Train loss 4.879, Val loss 4.806\n",
      "Epoch 2 (Global Step 000805): Train loss 4.799, Val loss 4.804\n",
      "Epoch 2 (Global Step 000810): Train loss 4.996, Val loss 4.812\n",
      "Epoch 2 (Global Step 000815): Train loss 4.880, Val loss 4.835\n",
      "Epoch 2 (Global Step 000820): Train loss 4.919, Val loss 4.808\n",
      "Epoch 2 (Global Step 000825): Train loss 4.790, Val loss 4.792\n",
      "Epoch 2 (Global Step 000830): Train loss 5.094, Val loss 4.782\n",
      "Epoch 2 (Global Step 000835): Train loss 4.988, Val loss 4.813\n",
      "Epoch 2 (Global Step 000840): Train loss 4.839, Val loss 4.809\n",
      "Epoch 2 (Global Step 000845): Train loss 4.778, Val loss 4.799\n",
      "Epoch 2 (Global Step 000850): Train loss 4.858, Val loss 4.792\n",
      "Epoch 2 (Global Step 000855): Train loss 4.974, Val loss 4.807\n",
      "Epoch 2 (Global Step 000860): Train loss 4.636, Val loss 4.783\n",
      "Epoch 2 (Global Step 000865): Train loss 4.857, Val loss 4.767\n",
      "Epoch 2 (Global Step 000870): Train loss 4.868, Val loss 4.764\n",
      "Epoch 2 (Global Step 000875): Train loss 4.747, Val loss 4.750\n",
      "Epoch 2 (Global Step 000880): Train loss 5.005, Val loss 4.748\n",
      "Epoch 2 (Global Step 000885): Train loss 4.565, Val loss 4.763\n",
      "Epoch 2 (Global Step 000890): Train loss 4.865, Val loss 4.759\n",
      "Epoch 2 (Global Step 000895): Train loss 4.917, Val loss 4.772\n",
      "Epoch 2 (Global Step 000900): Train loss 5.149, Val loss 4.764\n",
      "Epoch 2 (Global Step 000905): Train loss 4.587, Val loss 4.773\n",
      "Epoch 2 (Global Step 000910): Train loss 4.950, Val loss 4.762\n",
      "Epoch 2 (Global Step 000915): Train loss 4.787, Val loss 4.766\n",
      "Epoch 2 (Global Step 000920): Train loss 4.608, Val loss 4.805\n",
      "Epoch 2 (Global Step 000925): Train loss 4.636, Val loss 4.802\n",
      "Epoch 2 (Global Step 000930): Train loss 4.700, Val loss 4.768\n",
      "Epoch 2 (Global Step 000935): Train loss 4.796, Val loss 4.760\n",
      "Epoch 2 (Global Step 000940): Train loss 4.875, Val loss 4.747\n",
      "Epoch 2 (Global Step 000945): Train loss 4.765, Val loss 4.734\n",
      "Epoch 2 (Global Step 000950): Train loss 4.996, Val loss 4.732\n",
      "Epoch 2 (Global Step 000955): Train loss 4.894, Val loss 4.735\n",
      "Epoch 2 (Global Step 000960): Train loss 4.661, Val loss 4.734\n",
      "Epoch 2 (Global Step 000965): Train loss 4.813, Val loss 4.758\n",
      "Epoch 2 (Global Step 000970): Train loss 4.754, Val loss 4.747\n",
      "Epoch 2 (Global Step 000975): Train loss 4.676, Val loss 4.745\n",
      "Epoch 2 (Global Step 000980): Train loss 4.576, Val loss 4.735\n",
      "Epoch 2 (Global Step 000985): Train loss 4.765, Val loss 4.726\n",
      "Epoch 2 (Global Step 000990): Train loss 4.431, Val loss 4.711\n",
      "Epoch 2 (Global Step 000995): Train loss 4.884, Val loss 4.719\n",
      "Epoch 2 (Global Step 001000): Train loss 4.858, Val loss 4.736\n",
      "Epoch 2 (Global Step 001005): Train loss 4.829, Val loss 4.722\n",
      "Epoch 2 (Global Step 001010): Train loss 4.922, Val loss 4.717\n",
      "Epoch 2 (Global Step 001015): Train loss 4.884, Val loss 4.736\n",
      "Epoch 2 (Global Step 001020): Train loss 4.835, Val loss 4.754\n",
      "Epoch 2 (Global Step 001025): Train loss 4.653, Val loss 4.760\n",
      "Epoch 2 (Global Step 001030): Train loss 4.795, Val loss 4.743\n",
      "Epoch 2 (Global Step 001035): Train loss 4.928, Val loss 4.751\n",
      "Epoch 2 (Global Step 001040): Train loss 4.923, Val loss 4.739\n",
      "Epoch 2 (Global Step 001045): Train loss 4.700, Val loss 4.728\n",
      "Epoch 2 (Global Step 001050): Train loss 4.590, Val loss 4.749\n",
      "Epoch 2 (Global Step 001055): Train loss 5.105, Val loss 4.716\n",
      "Epoch 2 (Global Step 001060): Train loss 4.792, Val loss 4.730\n",
      "Epoch 2 (Global Step 001065): Train loss 5.044, Val loss 4.737\n",
      "Epoch 2 (Global Step 001070): Train loss 4.793, Val loss 4.739\n",
      "Epoch 2 (Global Step 001075): Train loss 4.669, Val loss 4.721\n",
      "Epoch 2 (Global Step 001080): Train loss 4.848, Val loss 4.708\n",
      "Epoch 2 (Global Step 001085): Train loss 4.890, Val loss 4.714\n",
      "Epoch 2 (Global Step 001090): Train loss 4.712, Val loss 4.721\n",
      "Epoch 2 (Global Step 001095): Train loss 4.726, Val loss 4.710\n",
      "Epoch 2 (Global Step 001100): Train loss 4.545, Val loss 4.728\n",
      "Epoch 2 (Global Step 001105): Train loss 4.933, Val loss 4.722\n",
      "Epoch 2 (Global Step 001110): Train loss 4.707, Val loss 4.738\n",
      "Epoch 2 (Global Step 001115): Train loss 4.706, Val loss 4.712\n",
      "Epoch 2 (Global Step 001120): Train loss 4.683, Val loss 4.678\n",
      "Epoch 2 (Global Step 001125): Train loss 4.710, Val loss 4.660\n",
      "Epoch 2 (Global Step 001130): Train loss 4.884, Val loss 4.686\n",
      "Epoch 2 (Global Step 001135): Train loss 4.459, Val loss 4.667\n",
      "Epoch 2 (Global Step 001140): Train loss 4.637, Val loss 4.678\n",
      "Epoch 2 (Global Step 001145): Train loss 4.828, Val loss 4.689\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPT2_124_model(CONFIG_GPT2_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "start_context = \"This is a test of the modified \"\n",
    "\n",
    "# set an extremely small epoch size for testing only\n",
    "num_epochs = 2\n",
    "train_losses, val_losses, tokens_seen = training_loop(model=model, \n",
    "                                                      data_train=loader_t, \n",
    "                                                      data_valid=loader_v, \n",
    "                                                      optimizer=optimizer, \n",
    "                                                      num_epochs=num_epochs,\n",
    "                                                      device=device, \n",
    "                                                      eval_frequency=5, \n",
    "                                                      eval_iter=5,\n",
    "                                                      start_context=start_context\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alternative to loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The perplexity is often considered more interpretable \n",
    "# because it can be understood as the effective vocabulary size \n",
    "# that the model is uncertain about at each step \n",
    "# (in the example below, that'd be perplexity number of tokens)\n",
    "perplexity = torch.exp(val_losses[-1])\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model + optimizer params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model only\n",
    "torch.save(model.state_dict(), \"my_gpt2_mdl.pth\")\n",
    "# save model + optimizers\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"my_gpt2_mdl_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load & use checkpoint\n",
    "\n",
    "torch.manual_seed(123)\n",
    "checkpoint = torch.load(\"my_gpt2_mdl_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model = GPT2_124_model(CONFIG_GPT2_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
