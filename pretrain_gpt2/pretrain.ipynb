{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training of GPT-2-124-million \n",
    "- pre-train on novel Anna Karenina\n",
    "- adaptations\n",
    "    - reduce the embedding size from 1024 to 256 to make sure the training loop will run locally\n",
    "    - change it back to 1024 when loading / using tre-trained weights\n",
    "- preprocess\n",
    "- modules and models\n",
    "    - define building-block modules\n",
    "    - define model\n",
    "- training loop\n",
    "- evaluation\n",
    "- inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys \n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "import importlib\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in raw text\n",
    "pdata = f\"{cwd[:-18]}traditional-NLP/data/\"\n",
    "sys.path.append(pdata)\n",
    "with open(f\"{pdata}anna.txt\" , 'r', encoding='utf-8') as f:\n",
    "    text_data = f.read()\n",
    "print(f\"The type of the raw text: {type(text_data)}\")\n",
    "print(f\"The beginning of raw text: \\n {text_data[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect raw text and tokens\n",
    "total_characters = len(text_data)\n",
    "print(f\"total num of characters in Anna Karenina: {total_characters}\")\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(f\"total num of tokens in Anna Karenina with BPE tokenizer: {total_tokens}\")\n",
    "# total num of characters in Anna Karenina: 1985223\n",
    "# total num of tokens in Anna Karenina with BPE tokenizer: 508206"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_GPT2_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch dataset dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloader\n",
    "\n",
    "class my_text_dataset(Dataset):\n",
    "\n",
    "    # initialize with n varg in\n",
    "    def __init__(self, raw_text:str, tokenizer, max_length:int, stride:int=1):\n",
    "        # create class attributes\n",
    "        self.input_tokens_x = []\n",
    "        self.target_tokens_y = []\n",
    "\n",
    "        # tokenize the enitre text \n",
    "        tokens = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # set y as stride number of tokens trailing x \n",
    "        for i in range(0, (len(tokens)-max_length), stride):\n",
    "            x_tmp = tokens[i : (i+max_length)]\n",
    "            y_tmp = tokens[(i+1) : (i+max_length+1)]\n",
    "            self.input_tokens_x.append(torch.tensor(x_tmp))\n",
    "            self.target_tokens_y.append(torch.tensor(y_tmp))\n",
    "\n",
    "    # overwrite the __len__() method to return number of rows in the dataset\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the number of rows / pairs of x-y sequences in the dataset\"\n",
    "        return len(self.input_tokens_x)\n",
    "    \n",
    "    # overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "        return self.input_tokens_x[idx], self.target_tokens_y[idx]\n",
    "\n",
    "def my_text_dataloader(raw_text:str, batch_size:int=4, max_length:int=256,\n",
    "                       stride:int=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # create dataset\n",
    "    dataset = my_text_dataset(raw_text, tokenizer, max_length, stride)\n",
    "\n",
    "    # create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split into T, V, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_characters = len(text_data)\n",
    "print(f\"total num of characters in Anna Karenina: {total_characters}\")\n",
    "prop_t, prop_v, prop_h = (0.8,0.1,0.1)\n",
    "split_idx_t, split_idx_v = int(prop_t * total_characters), int((prop_t+prop_v) * total_characters)\n",
    "print(f\"Split at character index {split_idx_t} between train and valid sets, and at {split_idx_v} betwee valid and hold sets\")\n",
    "\n",
    "d_train = text_data[:split_idx_t]\n",
    "d_valid = text_data[split_idx_t:split_idx_v]\n",
    "d_hold  = text_data[split_idx_v:]\n",
    "print(len(d_train), len(d_valid), len(d_hold))\n",
    "\n",
    "assert len(total_tokens * prop_t) > CONFIG_GPT2_124M[\"context_length\"], \"Not enough tokens for loader_t (training dataloader)\"\n",
    "assert len(total_tokens * prop_v) > CONFIG_GPT2_124M[\"context_length\"], \"Not enough tokens for loader_v (validation dataloader)\"\n",
    "assert len(total_tokens * prop_h) > CONFIG_GPT2_124M[\"context_length\"], \"Not enough tokens for loader_h (testing dataloader)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_t = my_text_dataloader(\n",
    "    raw_text=d_train,\n",
    "    batch_size=2, # this is only for learning purpose; in practice, batch_size >= 1024 is common\n",
    "    max_length=CONFIG_GPT2_124M[\"context_length\"],\n",
    "    stride=CONFIG_GPT2_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "loader_v = my_text_dataloader(\n",
    "    raw_text=d_valid,\n",
    "    batch_size=2,\n",
    "    max_length=CONFIG_GPT2_124M[\"context_length\"],\n",
    "    stride=CONFIG_GPT2_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# loader_h = my_text_dataloader(\n",
    "#     raw_text=d_hold,\n",
    "#     batch_size=2,\n",
    "#     max_length=CONFIG_GPT2_124M[\"context_length\"],\n",
    "#     stride=CONFIG_GPT2_124M[\"context_length\"],\n",
    "#     drop_last=False,\n",
    "#     shuffle=False,\n",
    "#     num_workers=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inspect loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in loader_t:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in loader_v:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in loader_t:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in loader_v:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modules and model\n",
    "- key components:\n",
    "    - tokenization - done in my_text_dataloader\n",
    "    - input embedding\n",
    "    - positional encoding\n",
    "    - dropout\n",
    "    - tansformer block\n",
    "        - layernorm\n",
    "        - multiheadattention CONFIG_GPT2_124M[\"n_heads\"] by CONFIG_GPT2_124M[\"n_layers\"]\n",
    "        - droppout+shortcut\n",
    "        - layernorm\n",
    "        - feedford\n",
    "        - dropout+shortcut\n",
    "    - layernorm\n",
    "    - output linear layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead_Causal_Attention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, n_heads, dropout_rate, qkv_bias=False):\n",
    "        # inherit from the nn.Module parent \n",
    "        super().__init__() \n",
    "\n",
    "        # make sure d_out is divisible by n_heads (modulous ope, remainder==0)\n",
    "        assert (d_out % n_heads == 0), \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.n_heads = n_heads\n",
    "        # floor division\n",
    "        self.d_head = d_out // n_heads\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # add the buffer to create mask and send it to device with the model \n",
    "        # but not update it\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(\n",
    "                torch.ones(context_length,context_length),\n",
    "                diagonal=1)\n",
    "        )\n",
    "        # add the dropout - object from nn.Dropout with param dropout_rate\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # add linear layer to combine heads out\n",
    "        self.combine_heads = nn.Linear(d_out, d_out)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # allowing batching: first is the batch dim of tensors\n",
    "        batch, n_tokens, d_in = x.shape\n",
    "\n",
    "        # initialize the w_query, w_key, w_value \n",
    "        # AND matmul with input embeddings x\n",
    "        queries = self.w_query(x)\n",
    "        keys = self.w_key(x)\n",
    "        values = self.w_value(x)\n",
    "\n",
    "        # ###### split weights for the heads ######\n",
    "        # dims from (batch, n_tokens, d_out) \n",
    "        # to (batch, n_tokens, n_heads, d_head)\n",
    "        queries = queries.view(batch, n_tokens, self.n_heads, self.d_head)\n",
    "        keys = keys.view(batch, n_tokens, self.n_heads, self.d_head)\n",
    "        values = values.view(batch, n_tokens, self.n_heads, self.d_head)\n",
    "        # then to (batch, n_heads, n_tokens, d_head)\n",
    "        queries = queries.transpose(1,2)\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        # ###### split weights for the heads ######\n",
    "\n",
    "        # attention score query @ key.T \n",
    "        # but remember the dims is (batch, n_tokens, n_heads, d_head) so transpose the last two\n",
    "        # !!! this computes dot product for each head !!!\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(2, 3))\n",
    "\n",
    "        # add causal attention masks \n",
    "        # computeation with trailing underscore are performed in-place\n",
    "        attention_scores.masked_fill_(\n",
    "            # change the mask to boolean (truncated to num of tokens)\n",
    "            self.mask.bool()[:n_tokens, :n_tokens],\n",
    "            # fill value when 1 in mask\n",
    "            -torch.inf\n",
    "        )\n",
    "\n",
    "        # attention weights = normalized attention scores\n",
    "        # scale the attention scores by the sqrt(embedding dimentsion) first \n",
    "        # to improve the training performance by avoiding small gradients.\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / (keys.shape[-1]**0.5),\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "        # apply dropout to attention weights \n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # calculate context vector attention weights @ values\n",
    "        # ###### combine across all heads  ######\n",
    "        # dims (batch, n_heads, n_tokens, d_head) to (batch, n_tokens, n_heads, d_head)\n",
    "        context_vectors = torch.matmul(attention_weights, values).transpose(1, 2)\n",
    "        context_vectors = context_vectors.contiguous().view(\n",
    "            batch, n_tokens, self.d_out\n",
    "        )\n",
    "        # Combines heads, where self.d_out= self.n_heads * self.d_head\n",
    "        context_vectors = self.combine_heads(context_vectors)\n",
    "        # ###### combine across all heads  ######\n",
    "\n",
    "\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.LayerNorm(emb_dim)\n",
    "# if we code it out it does the following\n",
    "# each mini-batch in the scenario is all the input embeddings of one context \n",
    "# mean and var came from calc across columns of the emsbeddings for each token\n",
    "# then scale and shif provides a linear transformation\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        # unlike buffers, Parameters will be updated\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.GELU()\n",
    "# when coding it out, it looks like the following\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed forward netword after multihead attention in each transformer block\n",
    "# why does this particular architecture have a 4 x expansion and shrinkage?\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(config[\"emb_dim\"], 4 * config[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * config[\"emb_dim\"], config[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Follows the architecture of GPT-2-124_million\n",
    "    Decoder only transformer\n",
    "\n",
    "    - tansformer block\n",
    "        - layernorm\n",
    "        - multiheadattention CONFIG_GPT2_124M[\"n_heads\"] by CONFIG_GPT2_124M[\"n_layers\"]\n",
    "        - droppout\n",
    "        - shortcut\n",
    "        - layernorm\n",
    "        - feedford\n",
    "        - dropout\n",
    "        - shortcut\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lnorm1 = LayerNorm(config[\"emb_dim\"])\n",
    "        self.mhca = Multihead_Causal_Attention(d_in=config[\"emb_dim\"], \n",
    "                                               d_out=config[\"emb_dim\"], \n",
    "                                               context_length=config[\"context_length\"], \n",
    "                                               n_heads=config[\"n_heads\"], \n",
    "                                               dropout_rate=config[\"drop_rate\"])\n",
    "        self.drop_out = nn.Dropout(p=config[\"drop_rate\"])\n",
    "        self.lnorm2 = LayerNorm(config[\"emb_dim\"])\n",
    "        self.ff = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define shortcut / residual connection for attenion block\n",
    "        residual_conn = x\n",
    "        # layer norm before attention\n",
    "        x = self.lnorm1(x)\n",
    "        # multihead causal attention\n",
    "        x = self.mhca(x)\n",
    "        # dropout \n",
    "        x = self.drop_out(x)\n",
    "        # shortcut / residual connection\n",
    "        x = x + residual_conn\n",
    "\n",
    "        # define residual for FeedForward block\n",
    "        residual_conn = x\n",
    "        # layer norm\n",
    "        x = self.lnorm2(x)\n",
    "        # feedforward\n",
    "        x = self.ff(x)\n",
    "        # drop_out\n",
    "        x = self.drop_out(x)\n",
    "        # residual connection\n",
    "        x = x + residual_conn\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it all together into a model \n",
    "class GPT2_124_model(nn.Module):\n",
    "    \"\"\"\n",
    "    - input embedding\n",
    "    - positional encoding\n",
    "    - dropout\n",
    "    - tansformer block\n",
    "    - layernorm\n",
    "    - output linear layer\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # input embedding\n",
    "        self.input_emb = nn.Embedding(num_embeddings=config[\"vocab_size\"],\n",
    "                                      embedding_dim=config[\"emb_dim\"])\n",
    "        # absolute positional encoding\n",
    "        self.pos_emb = nn.Embedding(num_embeddings=config[\"context_length\"], \n",
    "                                    embedding_dim=config[\"emb_dim\"])\n",
    "        # TO TRY RoPE\n",
    "        # from torchtune.modules import RotaryPositionalEmbeddings\n",
    "        # rope_dim = config[\"emb_dim\"]/config[\"n_head\"]\n",
    "        # self.pos_emb = RotaryPositionalEmbeddings(dim=rope_dim)\n",
    "\n",
    "        # dropout\n",
    "        self.drop_out = nn.Dropout(p=config[\"drop_rate\"])\n",
    "\n",
    "        # transformer block x n_layers times\n",
    "        self.transformer_block = nn.Sequential(\n",
    "            # unpack list comprehension to repeat transformer-block n_layers times\n",
    "            *[TransformerBlock(config) for _ in range(config[\"n_layers\"])]\n",
    "        )    \n",
    "\n",
    "        # layer norm\n",
    "        self.lnorm = LayerNorm(config['emb_dim'])\n",
    "\n",
    "        # final output layer\n",
    "        # expand tokens into logits in vocab_size dimensions\n",
    "        # do not add extra bias \n",
    "        self.out_layer = nn.Linear(in_features=config[\"emb_dim\"],\n",
    "                                   out_features=config[\"vocab_size\"],\n",
    "                                   bias=False)\n",
    "    \n",
    "    def forward(self, input_tokens):\n",
    "        batch_size, seq_len = input_tokens.shape\n",
    "        input_embeddings = self.input_emb(input_tokens)\n",
    "        posit_embeddings = self.pos_emb(torch.arange(seq_len, device=input_tokens.device))\n",
    "        # add positional encoding into input embedding \n",
    "        x = input_embeddings + posit_embeddings\n",
    "        # dropout\n",
    "        x = self.drop_out(x)\n",
    "        # transformer block\n",
    "        x = self.transformer_block(x)\n",
    "        # layer norm\n",
    "        x = self.lnorm(x)\n",
    "        # final output layer -> logits\n",
    "        logits = self.out_layer(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training loop\n",
    "- A typical training loop\n",
    "- loop through all training epochs\n",
    "    - within each epoch, loop through all batches (n_batches = train_size / batch_size)\n",
    "        - reset (from previous batch iter) the loss gradient to zero \n",
    "        - calculate loss on the current batch\n",
    "        - backpropagate loss gradient \n",
    "        - step to update weight and biases for next loop of training\n",
    "        - claculate training and validation losses\n",
    "        - visualize losses and sample texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval\n",
    "- loss calculation under the hood:\n",
    "    - get the logits from the transformer output layer\n",
    "    - convert logits to probablities with softmax\n",
    "    - get target probabilities \n",
    "    - take (-1) * log (probabilties) [ log(prob)<0 so maximize to 0 or (-1)* to minimize to 0 ]\n",
    "    - take the cross entropy loss between predicted and target \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)\n",
    "\n",
    "# The perplexity is often considered more interpretable \n",
    "# because it can be understood as the effective vocabulary size \n",
    "# that the model is uncertain about at each step \n",
    "# (in the example below, that'd be perplexity number of tokens)\n",
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss per batch\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "# function to compute loss per loader of data during training and validation\n",
    "# as sum of loss across all batches in the dataloader / number of batches\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        # iterate over all batches in the dataloader unless num_batches is otherwise given\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            # sum loss from each batch \n",
    "            total_loss += loss.item() \n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "with troch.no_grad():\n",
    "    loss_train = calc_loss_loader(data_loader=loader_t, model=model, device=device)\n",
    "    loss_valid = calc_loss_loader(data_loader=loader_v, model=model, device=device)\n",
    "\n",
    "print(f\"Training loss: {loss_train}\")\n",
    "print(f\"Validation loss: {loss_valid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
